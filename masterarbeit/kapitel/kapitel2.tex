%%% TeX-master: "../main.tex"
% kapitel2.tex
\chapter{Funktionstests graphischer Oberflächen}\label{chapter:introguitesting}


Dieses Kapitel befasst sich mit einigen in der Industrie verbreiteten bzw. \glqq{}State of the Art\grqq{}-Lösungen
für das Problem automatischer GUI-Tests, dazu mit in einem Artikel sowie einer Doktorarbeit
vorgestellten Methoden. Zunächst werden noch die Grundlagen graphischer Oberflächen und
der verschiedenen Methodiken erläutert, um diese zu testen.
Anschließend werden verschiedene Lösungen (für Java-Swing und andere) vorgestellt,
ihr jeweiliger Implementationsaufwand begutachtet sowie eventuelle Vor- und Nachteile aufgezeigt.
Auch wenn bereits im Ansatz ein Unterschied zum hier vorgestellten Konzept besteht, kann man dennoch
Aufwand und Nutzen sowie Vor- und Nachteile vergleichen.



\section{Grundlagen: Asynchrone graphische Oberflächen}


Programme mit einer graphischen Oberfläche stellen einen Sonderfall für
die Philosophie einer Implementation dar. Ein gewöhnliches Programm wird
gestartet, läuft und erledigt eine Aufgabe, und beendet sich nach
Durchführung dieser Aufgabe. Üblicherweise ist dies eine Datenverarbeitung
irgendeiner Art, und vielleicht gibt der Nutzer noch weitere Parameter
oder Argumente mit dem Startbefehl durch. Eine erste Erweiterung dieses
Konzepts ist, dass eine Anwendung anhalten kann, um den Nutzer nach einer
Eingabe zu fragen. Dies stellt allerdings noch keine besonderen Anforderungen
an die Anwendung -- schließlich muss nichts getan werden, während auf
die Nutzereingabe gewartet wird.

Eine Anwendung mit einer graphischen Oberfläche dahingegen steht vor
einem Dilemma: Es kann mit Sicherheit nur davon ausgegangen werden, dass ein
einziger Prozessor bzw. Kontrollfluss vorliegt. Es gibt aber im üblichen Fall
eine Vielzahl von Prozessen, die bearbeitet werden müssen. Hier eine exemplarische
Auflistung, in absteigender Wichtigkeit bzw. Priorität für den Prozessor
sortiert:

\begin{itemize}
  \item \textsc{Eingaben} getroffene Eingaben müssen aus entsprechenden Puffern ausgelesen werden
  \item \textsc{Oberfläche} die Darstellung der graphischen Oberfläche selbst
  \item \textsc{Reaktion} Eingaben verlangen häufig irgendeine Tätigkeit oder Reaktion
  \item \textsc{Aufgabe} Die Applikation hat nach wie vor eine Aufgabe zu erfüllen
\end{itemize}

Im Gegensatz zum klassichen Fall eines Programms müssen alle diese Tätigkeiten
mehr oder weniger gleichzeitig bearbeitet werden. Es wird gemeinhin als inakzeptabel
eingestuft, wenn eine graphische Oberfläche während einer Tätigkeit oder aufgrund
einer Eingabe nicht länger reagiert, bis diese beendet sind. Das Einzige, was
noch wichtiger ist als die Darstellung der aktuellen Oberfläche, ist die Entleerung
der Eingabepuffer. Wenn diese überlaufen, weil der Computer anderweitig beschäftigt
war, gehen Eingaben des Nutzers verloren. Dieser müsste daraufhin seine Eingabe
unterbrechen, auf eine Reaktion des Programms warten, herausfinden, ab wann seine
Eingabe verloren ging, und sie ab dann neu vornehmen. Dies ist sehr störend
und wenn überhaupt möglich zu vermeiden. Selbst eine Nichtreaktion der Nutzeroberfläche
ist als Preis akzeptabel, wenn es gleichzeitig dazu führt, dass alle getätigten
Eingaben auch vom Programm verarbeitet werden.

Als nächstgeringere Priorität folgen dann gewünschte Reaktionen auf diese Nutzereingaben.
Diese nehmen entweder nur Einfluss auf die Darstellung oder haben sonstigen
geringen Einfluss auf den Ablauf des Programms als solches -- als Beispiel genannt
sei das Eintippen einer Zeichenkette. Das Programm muss auf jeden Tastenanschlag
zeitnah reagieren und die entsprechenden Zeichen in der Oberfläche darstellen,
aber die einzelnen Eingaben führen nicht zu einer schwerwiegenden Zustandsänderung
oder einer Menge von Berechnungen im Programm. Eine solche Änderung tritt vermutlich 
erst ein, wenn eine Reihe solcher Eingaben getätigt und Einstellungen angepasst wurden, 
bevor schlussendlich ein \glqq{}schwerwiegender\grqq{} Prozess angestoßen wird,
der eigentliche Zweck des Programms.

Paradoxerweise ist nun die eigentliche Aufgabe des Programms von geringster Priorität. 
Sie erhält normalerweise Prozessorzeit, wenn keine der anderen Tätigkeiten dringend 
danach verlangt. Dies würde dazu führen, dass entweder die graphische Oberfläche
nicht mit den aktuellen Zustandsinformationen befüllt wird, keine Eingaben entgegen
nimmt oder auf diese nicht reagiert. Dies ist insbesondere dann ein Problem, wenn der
Nutzer beispielsweise beabsichtigte, den Hauptprozess zu unterbrechen oder zu pausieren.
Viele Applikationen befolgen diese Philosophie nicht, dementsprechend ist es nicht
möglich, einen einmal gestarteten Prozess vor vollständiger Beendigung zu stoppen,
ohne das ganze Programm vom Betriebssystem beenden zu lassen.

Das Betriebssystem ist für die Zuteilung der Prozessorzeit an Kontrollflüsse
zuständig, es existieren hierfür diverse Ansätze \cite{ArpaciDusseau14-Book}, 
und alle Betriebssysteme implementieren sogenannte \glqq{}Scheduler\grqq{}.
Dem obigen Prinzip folgend, haben die graphischen Nutzeroberflächen normalerweise
kritische Priorität bzw. \glqq{}Echtzeit\grqq{}-Anforderungen, das heisst, sie
werden anderen Prozessen vorgezogen. Moderne Betriebssysteme 
\cite{Stallings:2004:OS:993867} haben dennoch
Fortschrittsgarantien für alle lauffähigen Prozesse, die mittels verschiedener
Methodiken eingehalten werden. Unter Windows beispielsweise erhöht sich die
Priorität eines Prozesses, wenn Nutzereingaben darauf ausgeführt werden.
Gleichzeitig erhalten Prozesse, die am Ende der Warteschlange logieren,
alle paar Millisekunden (eine Ewigkeit in Computermaßstäben) Prozessorzeit,
die ihnen eigentlich nicht zustünde. So wird sichergestellt, dass alle Prozesse
auch vorankommen, obwohl einige wichtiger sind als andere.

Im Kontext des Testens von Software stellt dies eine Herausforderung dar,
weil gewöhnliches \glqq{}Debugging\grqq{} die Verfolgung von Kontrollflüssen
mithilfe von Breakpoints\footnote{url{https://en.wikipedia.org/wiki/Breakpoint}}
beinhaltet. Dieses Konzept fällt bei Abwesenheit eines monolithischen Programmflusses
aber auseinander. Es besteht nur noch die Möglichkeit, innerhalb des gewünschten
Unterprozesses auf hoffentlich korrekt empfangene Eingabesignale bzw. weitergeleitete
Befehle seitens der anderen Prozesse zu hoffen. Ein vollautomatischer Tester
muss also das Problem überwinden, dass eine über eine Anzahl von Kontrollflüssen
verteilte Applikation mit graphischer Oberfläche keine Informationen darüber
haben wird, welche Eingabe genau welche Reaktion hervorgerufen hat, da
die Prozesse entkoppelt miteinander kommunizieren.

Konkret heißt dies z.B., dass eine Eingabe, von der wir wissen, dass sie in der 
Öffnung eines neuen Fensters resultiert, keinerlei Zusicherungen seitens des 
Betriebssystems oder Computers genießt, innerhalb eines bestimmten Zeitrahmens 
gelesen und/oder verarbeitet zu werden. Wenn nun eine Eingabe mit unbekannter Wirkung 
getätigt wird, muss irgendwie erkannt werden können, dass eine Antwort zu einem 
beliebigen späteren Zeitpunkt kausal damit zusammenhängt.


\section{Grundlagen: Unterschiede Testverfahren}\label{section:testingapproaches}


Grundsätzlich haben Tests von graphischen Oberflächen das Ziel, sowohl die
Funktion als auch die Korrektheit zu überprüfen. Zum Bereich Funktion zählt,
dass alle vom Programm bereitgestellten Features oder Möglichkeiten mittels
Schaltflächen oder sonstigen komfortablen Möglichkeiten der Eingabe
intuitiv erreichbar und interaktiv sind. Dazu zählen z.B. die Nachfrage
der Applikation nach Parametern bzw. Argumenten für einen Funktionsaufruf,
die Bereitstellung wahrscheinlicher Standardeingabewerte, oder auch die
Überprüfung von Eingaben auf Zulässigkeit. So ist zum Beispiel die
Eingabe einer gewünschten Menge häufig nur für Werte größer als Null
und nur bis zu einer gewissen Höhe sinnvoll, aber ob ein Programmierer
auch entsprechende Tests bezüglich der Eingabe vorgesehen hat, ist
eine andere Frage.

Weiterhin sollten alle Schaltflächen, die eine Oberfläche bereitstellt,
bei Betätigung eine Rückmeldung geben -- selbst dann und insbesonders, wenn die verlangte
Funktion im Moment aus irgendeinem Grund nicht ausführbar ist.
Wenn ein Knopf momentan nicht funktioniert, muss dies entweder sichtbar gemacht werden,
oder eine Nachricht muss dem Nutzer erläutern, wieso der Knopfdruck nichts bewirkt.
\glqq{}Blinde\grqq{} Schaltflächen ohne jegliche Reaktion sind ein Funktionsdefizit.
Ein Nutzer kann schließlich üblicherweise nicht erkennen, ob und wieso eine Funktion
zu einem bestimmten Zeitpunkt bzw. unter gewissen Umständen nicht durchführbar ist
und wieso. Die Nutzeroberfläche muss diese Umstände zumindest wiedergeben
und idealerweise auch gleich erläutern, um den Nutzer zur korrekteren oder gewünschten
Bedienungsweise zu leiten.

Die Korrektheit einer Oberfläche bezeichnet z.B. das Öffnen eines erwarteten
Unterfensters oder die Ausführung der gewählten Funktion als Reaktion auf eine
Nutzereingabe. Es wird nicht einfach nur verlangt, dass etwas passiert, es soll
auch etwas sein, dass vom jeweiligen Schalter und dem Nutzungskontext abhängig ist.
Wenn dieser Effekt ausserhalb der graphischen Oberfläche zu finden ist, z.B.
als Änderung einer Datei im Betriebssystem, muss der Test ebenfalls die Grenzen
der getesteten Oberfläche verlassen und die gewünschte Änderung nachprüfen.

Da für solche Überprüfung die Mitwirkung des jeweiligen Betriebssystems notwendig
ist, welches je nach lokaler Ausführung unterschiedliche Eigenschaften für neue
Anwendungsfenster oder Dateisysteme aufweisen kann, zählt dies zu den am schwersten
zu validierenden Eigenschaften. Eine auf mehreren Betriebssystemen oder gar je
nach Version desselben Betriebssystems unterschiedlich agierende Applikation muss auch
in jeder denkbaren Konfiguration getestet werden. Hier gilt allerdings der
Grundsatz des Testens: \glqq{}Die Abwesenheit von Fehlern kann nicht bewiesen werden.\grqq{}
\footnote{E. Dijkstra, The Humble Programmer, ACM Turing Lecture 1972}

Der naive Ansatz eines Testverfahrens ist simpel. Ein menschlicher Tester startet
das Programm und arbeitet eine Liste von Funktionen bzw. Tätigkeiten ab, die das
Programm unterstützen muss. Er nutzt die graphische Oberfläche, um Eingaben zu tätigen,
und stellt sicher, dass Schaltflächen die vom Nutzer erwarteten oder zumindest
dokumentierte Verhaltensmuster aufweisen. Selbst wenn eine Oberfläche Probleme aufweist,
werden diese häufig als akzeptabel eingestuft, wenn eine gewünschte Funktion
bei Korrekter Bedienung ausführbar ist. Ein Beispiel für ein solches häufig nicht als kritisch 
eingestuftes Problem ist das temporäre Nichtantworten einer Programmoberfläche
aufgrund einer rechenintensiven internen Tätigkeit. Idealerweise ist eine
graphische Oberfläche tatsächlich nur eine Oberfläche und gibt Instruktionen als
verkapselte Signale an das unterliegende Programm weiter. Wenn also eine
in irgendeiner Form rechenintensive Anweisung erfolgt, sollte die Oberfläche
antwortfähig bleiben und lediglich vermerken, dass das unterliegende Programm
momentan beschäftigt ist und weitere Eingaben warten müssen.

Eine Erweiterung bzw. ein Komplement zu Funktionstests sind destruktive Ansätze.
Hierbei versucht ein Tester nicht, irgendwelche Funktionen korrekt durchzuführen,
sondern hat als einziges Ziel, die graphische Oberfläche in einen Fehlerzustand
zu überführen. Hierfür können z.B. schnelle, ziellose Eingaben dienen -- wildes Herumgeklicke
auf dem Bildschirm -- oder auch die gezielte Auswahl von falschen und ungültigen
Parametern und Eingaben, wo auch immer es geht. Die korrekt agierende
graphische Oberfläche muss dies alles verarbeiten können, ohne in einen nicht
mehr verlassbaren Fehlerzustand zu geraten. Dazu zählen z.B. Deadlocks,
Abstürze oder das Nichtakzeptieren folgender Eingaben.

\subsection{Automatisierbare Ansätze}

Menschliche Arbeitszeit ist allerdings teuer, diese Prüfverfahren sind üblicherweise
mit starker Wiederholung verbunden, und die Überprüfung selbst häufig trivial einfach.
Es bietet sich also an, dies zu automatisieren. Hierfür gibt es viele verfügbare Ansätze
und entsprechende Anwendungen. Es gibt sowohl quelloffene und kostenlose als auch proprietäre
und kommerzielle Lösungen. Grundsätzlich sind automatisierte Tests in zwei Kategorien einzuordnen:
Offline- oder Online-Tests. Hierzu mehr in Kapitel \ref{section:offoronlinetesting} auf Seite
\pageref{section:offoronlinetesting}. Menschliches Offline-Testen existiert natürlich auch,
es nennt sich beispielsweise \glqq{}Pair Programming\grqq{} oder \glqq{}Commit Review\grqq{}
und bedeutet im Grunde lediglich, dass ein oder mehrere andere Entwickler den geschriebenen Quellcode
lesen und nach Fehlern suchen.

Weitere Unterteilung automatisierter Tests erfolgt hauptsächlich durch Unterschiede
in der Bedienung und des jeweiligen Nutzers. Sogenannte \glqq{}Unit Tests\grqq{} werden
von den Entwicklern selbst verfasst und bezeichnen zusätzlichen Programmcode parallel
zum zu testenden Programm. Dieser zusätzliche Code soll lediglich die Funktion und Korrektheit
eines kleinen Bestandteils des Programms sicherstellen, insbesondere, wenn die Implementation
des Bestandteils geändert wird. Hierfür wird üblicherweise eine kleine Menge an Testeingabewerten
vorgesehen, für die erwartete bzw. korrekte Antworten definiert sind. Diese Testeingaben
werden dann beim Unit Test durchgeführt und die Antworten mit dem Soll verglichen.

Als konkretes Beispiel diene hier ein Programmiererwitz: Ein Tester betritt eine Kneipe.
Er bestellt ein Bier. Er bestellt -1 Biere. Er bestellt zehn Trilliarden Biere. Er bestellt n Biere.
Er verlässt die Kneipe. Er bestellt 2 Biere. Er betritt die Kneipe. Er bestellt ein Pferd. Er bestellt \glqq{}ioubiuzb\grqq{}.
Dieser Witz soll verdeutlichen, dass für gute Tests häufig unsinnige Eingaben notwendig sind,
die vom Programm dennoch auf korrekte Art und Weise behandelt werden müssen. So wäre es in diesem
Fall fehlerhaft, zehn Trilliarden Biere vorzubereiten, obwohl die Eingabe technisch gesehen zulässig erscheint.
Ebenso erfolgen Bestellungen, obwohl die Kneipe verlassen wurde. Ein korrekt agierendes
Programm sollte auf solche Widersprüche mit einer Fehlermeldung reagieren, aber nicht mit
fehlerhaftem Verhalten.

Entsprechende Tests gibt es auch für graphische Oberflächen, bei denen Code nicht zwangsläufig
einen Antwortwert zurückliefert. So kann z.B. ein Test darin bestehen, eine Schaltfläche zu betätigen,
und anschliessend zu prüfen, ob ein erwartetes Fenster geöffnet wurde. Hier wird die Problematik
komplexer, da bei einem korrekt verkapselten Programm das Öffnen eines neuen Fensters asynchron
und damit technisch gesehen zu einem nicht vorhersehbaren Zeitpunkt erfolgt. Für praktische Zwecke
kann allerdings vorgegeben werden, dass für eine korrekte Eingabeverarbeitung die Öffnung dieses 
Fensters innerhalb eines gewissen Zeitraums erfolgen muss.

Solche Tests müssen ebenfalls von Entwicklern implementiert werden, was einen erheblichen und
teuren Zeitaufwand bedeutet. Um dies zu optimieren, wurde ein neuer Ansatz erdacht: Anstatt
in Codeform vorgeben zu müssen, was das erwartete Verhalten einer graphischen Oberfläche ist,
wird ein naiver Testablauf durch einen Menschen bzw. Tester vom Computer aufgenommen. Diese Aufnahme der
vorgenommenen Eingaben kann dann beliebig oft wieder abgespielt werden. Die Ausgaben bzw.
Antworten des Programms wurden ebenso aufgenommen und können mit den Ausgaben eines späteren
Durchlaufs mit identischen Eingaben verglichen werden. Dies sind sogenannte 
\glqq{}Regressionstests\grqq{} \cite{regression}. Damit lässt sich vor allem sicherstellen,
dass bestehende Funktionalität einer Oberfläche durch spätere Änderungen nicht zerstört wird.

Die Vorteile sind offensichtlich: Es ist kein teurer Entwickler beteiligt, lediglich ein
erheblich günstigerer Tester, und ein einmal aufgenommener Testablauf kann ohne weitere
menschliche Beteiligung beliebig oft vorgenommen werden. Ein Nachteil ist allerdings, dass
nur die vom Tester aufgenommenen Aktionen getestet werden können, insbesondere können
meist die Parameter oder die Testreihenfolge nachträglich nicht verändert werden. Auch
muss bei einer Erweiterung oder Veränderung der bestehenden Oberfläche ein komplett neuer Test
erstellt werden, da der alte nicht mehr zutrifft bzw. Funktionen auslässt. Je nach genutztem
Test erhält ein Nutzer lediglich ein lapidares \glqq{}Test nicht bestanden\grqq{} oder
eine genaue Aufschlüsselung, welche Eingabe zu welchem Zeitpunkt zu einer fehlenden 
oder unerwarteten Reaktion geführt hat.

\subsection{Vollautomatische Ansätze}

Alle bisherigen Verfahren bezogen sich in irgendeiner Form auf menschliche Nutzung der
Oberflächen. Es gibt auch vollautomatische Ansätze, die nicht erfordern, dass ein Mensch
die korrekte Nutzung vorgibt. Diese nutzen meist Programmierschnittstellen aus,
auf denen graphische Oberflächen aufbauen. Diese Schnittstellen bieten Gemeinsamkeiten
über alle Programme hinweg, die zum vollautomatischen Testen verwendet werden können.
Dazu zählen insbesondere normierte Schaltflächen und sonstige Eingabemöglichkeiten
sowie die Erkennung sich öffnender Fenster. Ein vollautomatischer Test ist also zumindest
in der Lage, Schalter zu betätigen und neue Fenster zu beobachten.

Das Problem vollautomatischer Tests ist, dass Maschinen keine Intelligenz aufweisen,
um irgendeinen Kontext zu erkennen oder herstellen zu können. Ein Programm hat
keine Möglichkeit, festzustellen, ob ein sich öffnendes Fenster in einer Oberfläche
auch das gewünschte Fenster ist, ob es sich zum richtigen Zeitpunkt öffnet, oder
ob es überhaupt die korrekten Inhalte umfasst. Ebenso kann ohne Kontext nicht festgestellt
werden, dass ein bestimmter Schalter nur dann funktioniert, wenn vorher Parameter
in andere Eingabefelder eingegeben wurden, oder dass diese Parameter innerhalb bestimmter
Grenzen liegen müssen.

Für vollautomatische Tests bietet sich daher der bereits erwähnte destruktive Ansatz an.
Es gibt keine sinnvolle Möglichkeit, Funktion oder Korrektheit ohne Kontext zu testen,
aber um mögliche Fehler und Ausnahmen herbeizuführen, ist das auch unnötig.
Der Test versucht nun also, mit vielen, falschen, ungültigen oder auch unsinnigen
Eingaben das zu testende Programm zu überwältigen. Das Ziel ist es, nicht korrekt
behandelte Situationen zu finden, Kontrollflüsse also, in denen von falschen
Annahmen bezüglich des Programmzustands und der Eingaben ausgegangen wird.

Und obwohl solch ein vollautomatischer Test idealerweise keinerlei Angaben
über das zu testende Programm benötigt, sind in der Praxis doch Anpassungen
naheliegend. So macht es wenig Sinn, wieder und wieder einen Knopf zu testen,
der das Programm beendet und damit auch den Test. Eine völlig zufällige
Testanordnung würde zwar dennoch zwangsläufig irgendwann alle Möglichkeiten
ausgeschöpft haben, aber dieses simple Eingeständnis verringert die
Anzahl nötiger Testläufe erheblich.

Weiterhin kann jede Anwendung Bedienelemente enthalten, dessen Betätigung
den weiteren Test sinnlos macht -- als ein Beispiel sei eine Login-Maske genannt,
in der falsche Eingaben schlicht zur Beendigung des Programms führen.
Ein anderes Beispiel wären z.B. Schaltflächen wie
\glqq{}Projekt schließen\grqq{}, \glqq{}Alles löschen\grqq{} oder auch 
\glqq{}globaler thermonuklearer Krieg\grqq{}.


\section{Automatisierte GUI-Tests}\label{section:automatedguitesting}


\textbf{uispec4j} \footnote{\url{ https://github.com/UISpec4J/UISpec4J }} ist ein Java-Programm 
zum Test von graphischen Nutzeroberflächen
in Java. Es wird ein Programm implementiert, welches parallel zur zu testenden GUI läuft,
vorgegebene Eingaben vornimmt und im Anschluss einen gewünschten Zustand in den Elementen der GUI
überprüfen kann. Ein Problem ist dabei, dass nur oberflächliche Dinge geprüft werden können,
wie z.B. die Existenz oder das Fehlen eines fest definierten Fensters oder Elements. Weiterhin muss für jede
Prüfanfrage ein Entwickler ein entsprechendes Testsegment schreiben. Die Applikation muss also
zumindest doppelt implementiert werden, was in einem hohen Aufwand resultiert. Auch muss bei jeder
Refaktorisierung oder Änderung des Programms auch jeder anhängige Test angepasst werden.
Dies kann bei modernen Entwicklungsverfahren mit kurzen Iterations-Intervallen zu Problemen führen,
oder zumindest den Prozess verlangsamen.

\vspace{0.5cm}

\textbf{fest} \footnote{\url{ https://code.google.com/p/fest/ }} oder 
\glqq{}Fixtures for Easy Software Testing\grqq{} verfolgt denselben Ansatz.
Es agiert als Kontrollprogramm auf einer zu testenden Applikation und kann gewisse Prüfungen
auf dieser ausführen. Tatsächlich sind die beiden praktisch identisch, bauen sie doch beide
auf \textbf{JUnit} \footnote{\url{ http://junit.org/ }} auf und erweitern dieses.
uispec4j wird allerdings nicht mehr aktiv weiterentwickelt und bietet
beispielsweise keine Unterstützung für das Java Runtime Environment Version \glqq{}1.7\grqq{}
oder höher. Beide greifen auf die Java-Bibliothek \glqq{}Robot\grqq{} zurück,
die zur Durchführung von Tests einen menschlichen Nutzer simuliert. Dieser 
bewegt die Maus, klickt an vorprogrammierten Koordinaten mit dem Mauszeiger
(dies wird teilweise durch automatische Positionserkennung von definierten Elementen
der getesteten Oberflächen verbessert) und führt vordefinierte Tastatureingaben durch.


\vspace{0.5cm}

\textbf{QF-Test} \footnote{\url{ http://www.qfs.de/en/qftest/index.html }} 
bietet darüber hinaus eine eigene graphische Nutzeroberfläche
und ein sogenanntes \glqq{}Capture / Replay\grqq{}-System. Hierbei soll es Entwicklern erspart bleiben,
Tests als Programme zu implementieren, stattdessen bedient ein Tester durch die QF-Test-Maske
hindurch ein zu prüfendes Programm, während die Eingaben und Ausgaben mitgeschnitten werden.
Diese Aufnahme des Programmverhaltens gilt dann als \glqq{}Sollverhalten\grqq{} und wird in Folge
durch automatisierte Wiederholung verglichen und geprüft. Dieselbe Serie von Eingaben muss
bei Testdurchläufen eine identische Ausgabe zu diesem Soll hervorrufen, ansonsten
wird ein Fehler im Programm vermutet. Dieser Fehler kann auch eine gewünschte Programmveränderung sein.

Da allerdings hier ein Tester und nicht ein Entwickler die Tests erstellt und pflegt, ist dieses
Verfahren arbeitszeittechnisch wesentlich effizienter als vorherige Methoden. Ein Tester muss
nicht die Innereien des Programms verstehen oder verstehen können. Er muss das Programm lediglich
bedienen können. Ebenso sind keine Programmierkenntnisse notwendig. Der Tester folgt sogenannten
\glqq{}On-Screen\grqq{} Anweisungen, die Schritt für Schritt durch die korrekte Aufnahme einer
Nutzungsabfolge für spätere automatische Wiedergabe leiten. Das Resultat ist ein typischer
Regressionstest. Spätere Iterationen desselben Programms werden mit denselben Eingaben
bedient, und es wird unterstellt, dass auch dieselben (korrekten) Ausgaben erfolgen.
Stimmen gefundene Ausgaben in der Zukunft nicht mit der ursprünglichen Aufnahme überein, resultiert
dies in einer Fehlermeldung seitens des Testers. Dies tritt z.B. ein, wenn eine Refaktorisierung,
also eine Änderung bestehender Quellcode-Strukturen, in mehr als den erwarteten Änderungen
resultiert und bestehende Funktionalität zerstört oder beeinträchtigt. Dies tritt bei
gemeinsam genutzten Funktionen im Code schnell auf, insbesonders dann, wenn eine Implementation 
auf nicht dokumentierten Eigenarten bestimmter Anweisungen setzt, um ihr Ziel zu erreichen.

Undokumentierte Nebeneffekte überleben selten eine Refaktorisierung oder Reimplementierung
in derselben Form. Eine Lösung ist, das gewünschte Verhalten möglichst vollständig zu dokumentieren
und darüber hinaus mit Regressionstests zu überprüfen.



\subsection{Kontinuierliche Qualitätskontrolle von Webanwendungen auf Basis 
maschinengelernter Modelle}\label{ssection:windmueller}


Die \glqq{}Dissertation Kontinuierliche Qualitätskontrolle von Webanwendungen auf 
Basis maschinengelernter Modelle\grqq{} \cite{diss:windmueller} stellt ein neuartiges Konzept
zur Überprüfung des Verhaltens einer Applikation vor. Die Idee ist, mithilfe eines
lernenden Algorithmus nach Angluin \cite{angluin} einen Automaten bzw. ein komplettes Modell
der dem Algorithmus unbekannten, quasi \glqq{}Black Box\grqq{}, Anwendung zu erstellen und zu vergleichen.

Hierzu muss aber erwähnt werden, dass lediglich der das Modell erstellende Algorithmus nicht
weiss, was das Programm macht - seitens des Testers ist erheblicher Aufwand für die Implementation
von sogenannten Treibern notwendig, die jede mögliche Eingabe an die Anwendung bzw. jedes
mögliche Wort des Eingabealphabets abdeckt. Ebenso müssen die bei Webanwendungen normierten
Antworten auf die Eingaben gelesen und für Zustandsabbildung genutzt werden. Diese Treiber
sind hochspezifisch für eine Anwendung und müssen sogar für verschiedene Versionen derselben
Anwendung neu angepasst werden.

Zurück zum Prinzip: Als Resultat existieren nun die erlernten Modelle der Anwendung bzw. spezifischer
Versionen der Anwendung. Diese kann man nun mit einem Algorithmus vergleichen und Unterschiede
herausstellen. Solche entstehen letztendlich nur aus zwei Gründen: Die Anwendung wurde verändert
und es tritt eine gewünschte Funktionsänderung oder -erweiterung auf, oder ein neuartiger Fehler
wurde (im Vergleich zur letzten Version) erschaffen. Diese Differenz ist offensichtlich nur
aus dem Kontext heraus und nur durch einen Menschen zu verstehen.

Insofern handelt es sich um eine Form des Regressionstests \cite{regression} - bestehende bzw.
vorherige Funktion wird mit einem Momentanzustand verglichen und Diskrepanzen aufgedeckt. Der
Ansatz hat dementsprechend dieselben Nachteile - jede gewünschte Änderung des Systems muss eine
Neueinstellung oder Neuerstellung der Testkonfiguration nach sich ziehen. Bei häufigen
Änderungen kann der Aufwand der Testerstellung schnell dem Entwicklungsaufwand gleichkommen bzw.
diesen sogar überschreiten.

Der Vorteil ist natürlich genauso, dass selbst geringfügige Abweichungen im Verhalten sofort
auffallen und zur Analyse markiert werden. So wurden beim Praxiseinsatz auf verschiedenen
Webanwendungen und ihren Versionskontrollsystemen bis dato unerkannte Programmfehler entdeckt.
Diese wurden durch Eigenarten und Konzepte der bestehenden Tests maskiert und unsichtbar.
Eine offensichtliche Schlussfolgerung ist, dass mehr und verschiedene Tests zu besseren
Ergebnissen führen - das Problem liegt, wie immer, im Aufwand.

Verglichen mit einem gewöhnlichen Regressionstest erspart dieser Ansatz dem Nutzer
bzw. Entwickler die Definition der Zustandsfolgen. Zu beachten ist allerdings wieder:
Es gibt für den Algorithmus kein richtig oder falsch, nur Unterschiede zwischen Versionen.
Ein normaler GUI-Test definiert üblicherweise eine erwartete Antwort oder Reaktion auf einen
bestimmten Knopfdruck. Hier würde so lange kein \glqq{}Problem\grqq{} auftreten, wie das Verhalten eines
Programms konsistent ist, selbst wenn dieses Verhalten an sich eigentlich fehlerhaft ist.

Es gibt noch ein weiteres für das hier vorgestellte Konzept interessantes Feature,
den sogenannten reuse-Filter. Da ein Automat mit einer endlichen Zustandsmenge unterstellt
wird und gelernt werden soll (der Ansatz würde versagen, wenn das Programm nicht in dieses
Schema fällt), muss eine bestimmte Eingabe von einem bestimmten Zustand aus erfolgen.
Diesen Zustand allein durch Eingaben wieder zu erreichen - unter Umständen nach einem Programmneustart -
kann erhebliche Verzögerungen im Testablauf hervorrufen.

Die Idee ist folglich, Zustände des
zu testenden Programms zu sichern und dann aus dem Speicher wiederherzustellen. Dies setzt
natürlich voraus, dass dies möglich ist und man es für das Programm implementiert.
Für den hier vorgestellten Ansatz und allgemeiner eine Java Virtual Machine ist dies nicht
ohne weiteres möglich. Java-Programme sind zustandsmäßig unüberschaubar und können auch nicht
grundsätzlich in einem bestimmten Zustand abgespeichert werden. Die Alternative
eines kompletten virtuellen Rechners mit einer eingefrorenen JVM darin wäre
zumindest sehr aufwändig und vermutlich nicht sehr performant.

Weitere mögliche Nebeneffekte, die bei den getesteten Programmen nicht vorkamen, sind
Zustandsveränderungen auf einer persistenten Datenbank oder einem Speichermedium, die sogar
Beendigung des zu testenden Programms selbst überdauern. Hierfür müsste eine externe
Lösung gefunden werden, zum Beispiel das Sichern eines gesamten Systemzustands in einer
virtuellen Maschine und das jeweilige Neustarten eines Testablaufs von diesem gesicherten
Zustand aus.

\vspace{0.5cm}

Im Vergleich mit Dr. Windmüllers Ansatz wird das hier vorgestellte Konzept erheblich weniger
Kontextinformationen benötigen. So lange keine programmspezifischen, mit der Java-Swing-API
inkompatiblen Implementationen vorliegen, sollte ein Testdurchlauf ohne jede Information abseits
des Startpunktes möglich sein. Es gibt kein Eingabealphabet, welches erst für ein zu testendes
Programm definiert werden müsste.

Des Weiteren haben Java-GUI-Anwendungen den Nachteil, nicht auf jede Eingabe antworten oder gar
reagieren zu müssen. Eine Reaktion tritt nicht einmal im selben Kontrollfluss auf, obwohl Swing
konzeptuell monolithisch abläuft. Ebenso ist die Definition eines Zustandes selbst nicht trivial,
der interne Programmzustand ist unüberschaubar. Lediglich das Auftreten und die Erscheinung neuer
Fenster und graphischer Programmkomponenten kann beobachtet und verzeichnet werden.



\subsection{An Empirical Study of the Robustness of Windows NT Applications Using Random Testing}\label{ssection:windmueller}


Diese etwas ältere Arbeit von Forrester und Miller \cite{winNTforrester} befasst sich 
mit dem Verhalten einer Auswahl von Anwendungen im Betriebssystem Windows, wenn zufällige 
Daten als Eingaben verwendet wurden.
Es wurden sowohl Anwendungen mit und ohne grafischer Nutzeroberfläche getestet. Der Ansatz hierbei
ist Black-Box; es gibt keinerlei Wissen über Inhalt, Zweck oder Verhalten der zu testenden Software.
Als zufällige Eingaben dienen sowohl gültige Signale von Tastatur und Maus, wie ein Nutzer sie erzeugen
könnte, sowie Windows-spezifische, sogenannte \glqq{}Win32\grqq{}-Signale. Diese sind auch im aktuellen Windows 10
nach wie vor im Einsatz, aufgrund der Abwärtskompatibilität vermutlich sogar in nahezu identischer Form.

Forrester und Miller nennen ihr Vorgehen \glqq{}Fuzz Testing\grqq{}. Vor Windows wandten sie dasselbe Verfahren
in zwei Vorgänger-Arbeiten auf Linux-Anwendungen an. Bei gängigen Applikationen zeigte sich, dass zwischen
ein Viertel und ein Drittel der geprüften Anwendungen nicht mit den zufälligen Eingabedaten zurecht kam.
Das einzige Kriterium für den \glqq{}Erfolg\grqq{} ist die Abnahme der Eingabe sowie ein ordnungsgemäßes Beenden
des Programms -- selbst wenn dies lediglich eine sofortige Ausgabe einer Fehlermeldung ist.

Die Eingabe von zufälligen Maus- und Tastatursignalen ist definitiv einem realen Anwendungsfall zuzuordnen,
dieser Fall könnte schliesslich genau so auch auftreten. Die zufälligen Win32-Signale testen eher die
allgemeine Stabilität bzw. Sicherheit, die Fehlererkennung, eines Programms.

Eine Eingabe mittels Maus oder Tastatur löst zunächst eine Prozessor-Unterbrechung aus. Die Unterbrechung
leitet die Eingabedaten an den jeweiligen Gerätetreiber weiter, welcher den Inhalt der Nachricht ausliest
(welche Taste wurde gedrückt, wo befindet sich der Mauszeiger etc.). Dies löst ein Win32-Ereignis
aus. Das Betriebssystem stellt dann fest, welche Applikation das Ziel der Eingabe war, und kopiert
das Ereignis in den Ereignis-Eingang dieser Applikation. Anwendungen haben üblicherweise eine interne
Endlossschleife, welche diesen Eingang regelmäßig auf neue Nachrichten überprüft, diese ausliest und das
Programm entsprechend reagieren lässt. Obwohl im Normalfall davon ausgegangen werden kann, dass ein System
nur gültige Win32-Nachrichten verschickt, sollte ein Programmierer nicht davon absehen, dies auch
zu kontrollieren. Ein Angreifer könnte ansonsten ein Fehlverhalten des Programms bei ungültigen Eingaben
ausnutzen, es könnten Sicherheitslücken auftreten ö.Ä..

Zu beachten ist allerdings, dass \glqq{}Fuzz\grqq{} eine gewaltige Anzahl von Eingaben praktisch gleichzeitig tätigt
(zehntausende). Man könnte zu Recht argumentieren, dass ein normaler Anwendungsfall eine solche Menge von
Eingaben in einer kurzen Zeit nicht vorsieht, und damit vorsätzlich interne Puffer zur 
Verarbeitung von Eingaben überlastet werden
könnten. Modernere Versionen derselben Applikationen bzw. desselben Betriebssystems könnten diesen Fall 
unter Umständen besser abfangen.

\vspace{0.5cm}


Zu den Ergebnissen: Getestet wurden verschiedene bekannte Applikationen der Firmen Adobe und Microsoft sowie Mozilla -- es
finden sich der Acrobat Reader, Office, Internet Explorer sowie der Netscape Navigator 
(Vorläufer von Firefox) in der Liste. Getestet wurde unter Windows NT und Windows 2000.

21\% der getesteten Applikationen stürzten ab, wenn zufällige, gültige Maus- und Tastatureingaben 
getätigt wurden. Weitere 24\% versagten in Form von Endlosschleifen. Dies ist eine Fehlerrate von 45\%
allein für völlig legale und im Zweifelsfall möglicherweise auftretende Eingaben.

Im Fall der zufälligen (auch ungültigen) Win32-Signale lag die Fehlerrate bei nahezu 100\%. Dies ist
laut den Autoren, die den Quellcode einiger Applikationen einsehen konnten, damit zu erklären,
dass Programmierer von der Verlässlichkeit der empfangenen Signale ausgehen (diese Garantie ist offensichtlich
nicht gegeben). Die Autoren nennen dies eine grobe Schwachstelle der Win32-API, insbesondere bezüglich
Sicherheit, da schließlich jedes Programm der Systemebene diese Win32-Signale beliebiger Zusammensetzung
an andere Programme versenden kann. Dies könnte in neueren Windows-Versionen anders sein, das Betriebssystem
könnte eine automatische Fehlerkorrektur der Win32-Nachrichten vornehmen.

Für das hier vorgestellte Konzept stellen sich im Bezug auf diese Arbeit einige Fragen:
Wie schnell sollten Eingaben erfolgen? Müssen Eingaben zwangsläufig funktional korrekt erfolgen?
Auf welcher Abstraktionsebene erfolgen die Eingaben und welche Möglichkeiten zur Datenmanipulation
gibt es überhaupt? Wird die Nachstellung eines bösartigen Angriffs beabsichtigt?
