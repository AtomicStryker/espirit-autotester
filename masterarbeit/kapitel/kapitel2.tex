%%% TeX-master: "../main.tex"
% kapitel2.tex
\chapter{Funktionstests graphischer Oberflächen}\label{chapter:introguitesting}


Dieses Kapitel stellt zunächst einige für graphische Nutzeroberflächen wichtige 
Grundlagen und Konzepte vor, speziell bezüglich der für Tests bzw. deren Umsetzung
relevanten Asynchronität und der daraus folgenden Herausforderungen.
Im folgenden Abschnitt werden die verschiedenen Ansätze für diese Tests
und ihre jeweiligen Einsatzgebiete und Limitationen vorgestellt und verglichen.
Schließlich befasst sich das Kapitel mit einigen in der Industrie 
verbreiteten bzw. \glqq{}State of the Art\grqq{}-Lösungen
für das Problem automatischer GUI-Tests, dazu mit in einem Artikel sowie einer Doktorarbeit
vorgestellten Methoden. Zunächst werden noch die Grundlagen graphischer Oberflächen und
der verschiedenen Methodiken erläutert, um diese zu testen.
Anschließend werden verschiedene Lösungen (für Java-Swing und andere) vorgestellt,
ihr jeweiliger Implementationsaufwand begutachtet sowie eventuelle Vor- und Nachteile aufgezeigt.
Auch wenn bereits im Ansatz ein Unterschied zum hier vorgestellten Konzept besteht, kann man dennoch
Aufwand und Nutzen sowie Vor- und Nachteile vergleichen.



\section{Grundlagen: Asynchronität graphischer Oberflächen}


Programme mit einer graphischen Oberfläche stellen einen Sonderfall für
die Philosophie einer normalen Implementation dar. Ein gewöhnliches Programm wird
gestartet, läuft und erledigt eine Aufgabe, und beendet sich nach
Durchführung dieser Aufgabe selbst. Üblicherweise ist dies eine Datenverarbeitung
irgendeiner Art, die Daten liegen in passender Form vor oder werden 
als Datenstrom eingeholt, und vielleicht gibt der Nutzer noch weitere Parameter
oder Argumente mit dem Startbefehl weiter. Eine erste Erweiterung dieses
Konzepts ist, dass eine Anwendung anhalten kann, um den Nutzer nach einer
Eingabe zu fragen. Dies stellt allerdings noch keine besonderen Anforderungen
an die Anwendung -- schließlich muss nichts getan werden, während auf
die Nutzereingabe gewartet wird.

Eine Anwendung mit einer graphischen Oberfläche hingegen steht vor
einem Dilemma: Es kann mit Sicherheit nur davon ausgegangen werden, dass eine
einzige CPU bzw. ein einziger Kontrollfluss vorliegt. Es gibt aber im üblichen Fall
eine Vielzahl von Prozessen, die gleichzeitig bearbeitet werden müssten. Hier eine exemplarische
Auflistung, in absteigender Wichtigkeit bzw. Priorität für den Prozessor
sortiert:

\begin{itemize}
  \item \textsc{Eingaben} Getroffene Eingaben müssen aus entsprechenden Puffern ausgelesen werden
  \item \textsc{Oberfläche} Die Darstellung der graphischen Oberfläche selbst
  \item \textsc{Reaktion} Eingaben verlangen häufig irgendeine Tätigkeit oder Reaktion
  \item \textsc{Aufgabe} Die Applikation hat nach wie vor eine Aufgabe zu erfüllen
\end{itemize}

Im Gegensatz zum klassichen Fall eines Programms müssen alle diese Tätigkeiten
mehr oder weniger gleichzeitig bearbeitet werden. Es wird gemeinhin als inakzeptabel
eingestuft, wenn eine graphische Oberfläche während einer Tätigkeit oder aufgrund
einer Eingabe nicht länger reagiert, bis diese beendet sind. Das Einzige, was
noch wichtiger ist als die Darstellung der aktuellen Oberfläche, ist die Entleerung
der Eingabepuffer. Wenn diese überlaufen, weil der Computer anderweitig beschäftigt
war, gehen Eingaben des Nutzers verloren. Dieser müsste daraufhin seine Eingabe
unterbrechen, auf eine Reaktion des Programms warten, herausfinden, ab wann seine
Eingabe verloren ging, und sie ab dann neu vornehmen. Dies ist sehr störend
und wenn überhaupt möglich zu vermeiden. Selbst eine Nichtreaktion der Nutzeroberfläche
ist als Preis akzeptabel, wenn es gleichzeitig dazu führt, dass alle getätigten
Eingaben auch vom Programm verarbeitet werden.

Als nächstgeringere Priorität folgen dann gewünschte Reaktionen auf diese Nutzereingaben.
Diese nehmen entweder nur Einfluss auf die Darstellung oder haben sonstigen
geringen Einfluss auf den Ablauf des Programms als solches -- als Beispiel genannt
sei das Eintippen einer Zeichenkette. Das Programm muss auf jeden Tastenanschlag
zeitnah reagieren und die entsprechenden Zeichen in der Oberfläche darstellen,
aber die einzelnen Eingaben führen nicht zu einer schwerwiegenden Zustandsänderung
oder einer Menge von Berechnungen im Programm. Eine solche Änderung tritt vermutlich 
erst ein, wenn eine Reihe solcher Eingaben getätigt und Einstellungen angepasst wurden, 
bevor schlussendlich ein \glqq{}schwerwiegender\grqq{} Prozess angestoßen wird,
der eigentliche Zweck des Programms.

Paradoxerweise ist nun die eigentliche Aufgabe des Programms von geringster Priorität. 
Sie erhält normalerweise Prozessorzeit, wenn keine der anderen Tätigkeiten dringend 
danach verlangt. Dies würde dazu führen, dass entweder die graphische Oberfläche
nicht mit den aktuellen Zustandsinformationen befüllt wird, keine Eingaben entgegen
nimmt oder auf diese nicht reagiert. Dies ist insbesondere dann ein Problem, wenn der
Nutzer beispielsweise beabsichtigt, den Hauptprozess zu unterbrechen oder zu pausieren.
Viele Applikationen befolgen diese Philosophie nicht, dementsprechend ist es nicht
möglich, einen einmal gestarteten Prozess vor vollständiger Beendigung zu stoppen,
ohne das ganze Programm vom Betriebssystem beenden zu lassen.

Das Betriebssystem ist für die Zuteilung der Prozessorzeit an Kontrollflüsse
zuständig, es existieren hierfür diverse Ansätze \cite{ArpaciDusseau14-Book}, 
und die bekanntesten Betriebssysteme implementieren sogenannte \glqq{}Scheduler\grqq{}.
Dem obigen Prinzip folgend, haben die graphischen Nutzeroberflächen normalerweise
kritische Priorität bzw. \glqq{}Echtzeit\grqq{}-Anforderungen, das heißt, sie
werden anderen Prozessen vorgezogen. Moderne Betriebssysteme 
\cite{Stallings:2004:OS:993867} haben dennoch
Fortschrittsgarantien für alle lauffähigen Prozesse, die mittels verschiedener
Methodiken eingehalten werden. Unter Windows beispielsweise erhöht sich die
Priorität eines Prozesses, wenn Nutzereingaben darauf ausgeführt werden.
Gleichzeitig erhalten Prozesse, die am Ende der Warteschlange logieren,
alle paar Millisekunden (eine Ewigkeit in Computermaßstäben) Prozessorzeit,
die ihnen eigentlich nicht zustünde. So wird sichergestellt, dass alle Prozesse
auch vorankommen, obwohl einige wichtiger sind als andere.

Im Kontext des Testens von Software stellt dies eine Herausforderung dar,
weil gewöhnliches \glqq{}Debugging\grqq{} die Verfolgung von Kontrollflüssen
mithilfe von Breakpoints\footnote{ url{ https://en.wikipedia.org/wiki/Breakpoint } }
beinhaltet. Dieses Konzept fällt bei Abwesenheit eines monolithischen Programmflusses
aber auseinander. Es besteht nur noch die Möglichkeit, innerhalb des gewünschten
Unterprozesses auf hoffentlich korrekt empfangene Eingabesignale bzw. weitergeleitete
Befehle seitens der anderen Prozesse zu hoffen. Ein vollautomatischer Tester
muss also das Problem überwinden, dass eine über eine Anzahl von Kontrollflüssen
verteilte Applikation mit graphischer Oberfläche keine Informationen darüber
haben wird, welche Eingabe genau welche Reaktion hervorgerufen hat, da
die Prozesse entkoppelt miteinander kommunizieren.

Konkret heißt dies z.B., dass eine Eingabe, von der wir wissen, dass sie in der 
Öffnung eines neuen Fensters resultiert, keinerlei Zusicherungen seitens des 
Betriebssystems oder Computers genießt, innerhalb eines bestimmten Zeitrahmens 
gelesen und/oder verarbeitet zu werden. Wenn nun eine Eingabe mit unbekannter Wirkung 
getätigt wird, muss irgendwie erkannt werden können, dass eine Antwort zu einem 
beliebigen späteren Zeitpunkt kausal damit zusammenhängt.


\section{Grundlagen: Black-Box, White-Box}


Im Bereich der Softwaretests allgemein unterscheidet man zwischen
sogenannten \glqq{}Black-Box\grqq{} und 
\glqq{}White-Box\grqq{}-Ansätzen. Letzterer wird im
deutschen manchmal auch Glasbox genannt. Der Begriff stammt
aus dem Militär, eroberte feindliche Funkgeräte durften wegen eventueller
Sprengfallen nicht geöffnet werden. Diese Fallen sollten sicherstellen,
dass eventuelle Geheimnisse bezüglich des Kommunikationswegs wie
z.B. eine Verschlüsselungsmethode nicht eingesehen werden konnten.
Also ist eine \glqq{}Black-Box\grqq{} ein Konstrukt nicht einsehbaren
Inhalts. In der Softwareentwicklung wird der Begriff genutzt,
um Programme oder Bibliotheken zu bezeichnen, für die kein
Quellcode vorliegt bzw. deren interne Funktionalität nicht einsehbar 
ist\cite{bookblackbox}. Insbesondere im Fall der objektorientierten
Programmierung ist dies ein schwerer Nachteil. Eine solche Black-Box
kann während Tests nur mittels Ein- und Ausgaben mit der Umwelt bzw. der
Testumgebung kommunizieren, und es kann dementsprechend
schwierig werden, Fehler zu finden bzw. Korrektheit sicherzustellen.
Insbesondere, wenn diese Ein- und Ausgaben nur in Form von Objektinstanzen
geschehen soll, ist Einsicht in die Funktionsparameter bzw. eine sichtbare
Programmschnittstelle zwingend erforderlich. Die Hürde ist
je nach genutzter Programmiersprache unterschiedlich hoch,
da gewisse Programmiersprachen fast vollständig dekompiliert werden
können -- hierzu zählt Java -- während andere, wie C oder C++,
von den Compilern stark optimiert in Maschinencode umgesetzt werden,
und nicht mehr eindeutig oder für Menschen leicht verständlich 
wieder in Quellcode zurück übersetzt werden können.

Beim Black-Box-Test ist ein Hauptproblem, dass Fallunterscheidung
zum Testen aller möglichen Eingabeklassen eigentlich voraussetzt,
die möglichen Fälle zu kennen. Man würde im Quellcode nachlesen,
welche Form einer Eingabe einen gewissen Codeblock ausführt bzw.
welche Eingabe einen bestimmten Kontrollfluss zur Folge hat, und einen Testfall
mit genau dieser Eingabeform vorsehen -- z.B. einer ganzen Zahl, einer
Zeichenkette bestimmter Länge, oder eine Serie von Bits in einem
bestimmten Format. Im Fall des Black-Box-Tests existiert diese Information
nicht, es gibt eine unbekannte Anzahl von Eingabeklassen. Folglich
muss für einen umfassenden Test jede Form von zulässiger
Eingabeklasse ausprobiert werden, und im Fall von Robustheitstests
auch unzulässige Eingaben.

Auch der Zustand einer solchen Black-Box kann nur über ihre Ausgaben
implizit überwacht werden. Ansätze wie der von Angluin\cite{angluin}
haben das Ziel, die wiederholbaren Zustände innerhalb einer Black-Box
durch Kontrolle der Eingaben und Überwachung der Ausgaben abzubilden.
Wenn z.B. auf aufeinanderfolgende,
identische Eingaben eine abweichende Antwort erfolgt, muss die Black-Box
einen internen Zustand besitzen, der sich zwischen den Eingaben verändert hat.
Diese Änderung muss durch die Eingaben eingetreten sein -- für die Zwecke
des Black-Box-Tests existiert außer der Box und der Testumgebung nichts,
die Umwelt ist isoliert. Ein Problem dieses Ansatzes ist, dass man nie
sicher sein kann, ob eine Abfolge von immer derselben Eingabe oder
eine bestimmte, sehr lange Kombination von Eingaben nicht irgendwann
doch eine Zustandsänderung zur Folge hat. Auch ist es im Fall von
Zeichenketten wenig praktikabel, jede mögliche Zeichenkette
als Eingabe auszuprobieren. Die Zahl der Möglichkeiten ist viel zu hoch,
als dass man jemals einen irgendwie sinnvollen Anteil prüfen könnte.

Als Kompromiss werden häufig typische Eingabeklassen geraten und
jeweils einer oder mehrere Vertreter getestet. So ist es beispielsweise 
selten sinnvoll, die Zahlen von null bis einhundert auszuprobieren.
Sinnvolle Vertreter wären beispielsweise null und eins wegen ihrer
besonderen mathematischen Eigenschaften, eine beliebige einstellige
Zahl über eins, sowie eine beliebige zweistellige Zahl und einhundert.
Sollte keine besondere Funktionalität im Programm vorliegen, die
gerade auf irgendwelche Zahlen des Intervalls von null bis hundert
abzielt, sind damit alle üblicherweise zu erwartenden Eingabeklassen 
abgedeckt. Eine solche Prüfung lässt sich durch zufällige Elemente
bei der Auswahl der Eingaben und Wiederholung verbessern,
es können mehr potentielle Kontrollflüsse getestet werden.
Black-Box-Tests dienen häufig dazu, ein Programm oder eine
Bibliothek auf eine bestimmte Spezifikation hin zu testen.
Es gibt also vorgegebene Eingabeklassen und erwartete Antworten.
Gibt eine Black-Box für jede Eingabe die korrekte Antwort,
gilt der Test als bestanden -- unabhängig davon, wie die
Antwort intern implementiert wurde. Sekundäre Spezifikationen
wie die Geschwindigkeit der Antwort könnten ebenfalls eine
Rolle spielen.

Im Gegensatz dazu steht der White-Box-Test\cite{bookwhitebox}.
Hier liegt der Quellcode vor oder die Instruktionen können zumindest
eingesehen werden (zum Beispiel mittels eines Dekompilierers).
Ein Test ist also nicht auf die Schnittstellen angewiesen, sondern
kann direkt auf die internen Funktionen des zu prüfenden
Konstrukts zugreifen. Auch müssen Kontrollflüsse nicht geraten,
rekonstruiert oder erahnt werden, sondern sind direkt einsehbar.
Tests werden dementsprechend häufig so erstellt, dass jeder
Codepfad bzw. Kontrollfluss des Programms zumindest ein Mal
durchlaufen wird, oder dass jede Zeile Programmcode zumindest
ein Mal ausgeführt wird. Es ist gewissermaßen die online-Variante
einer statischen Analyse. Der Aufwand ist gegenüber Black-Box-Tests
erheblich geringer. Mögliche Nachteile des White-Box-Tests gegenüber
Black-Box sind zum Beispiel die Nichterfüllung der Spezifikation,
wenn im Quellcode schlicht keine passende Implementation vorliegt,
oder auch die Vermeidung von potentiell problematischen Eingaben.
Auch müssen so angepasste Tests bei Änderungen am Quellcode
auch angepasst werden, um die Abdeckung zu bewahren.

In der Praxis wird häufig eine Kombination von beiden Ansätzen
genutzt -- Black-Box zum Prüfen einer erforderlichen Spezifikation
mit einer ganzen Eingabeklasse und White-Box, um die Korrektheit
einer Implementation sicherzustellen. In Fällen, wo kein Quellcode
vorliegt, muss natürlich grundsätzlich ein Black-Box-Test eingesetzt werden.



\section{Grundlagen: Unterschiede Testverfahren}\label{section:testingapproaches}


Grundsätzlich haben Tests von graphischen Oberflächen das Ziel, sowohl die
Funktion als auch die Korrektheit zu überprüfen. Zum Bereich Funktion zählt,
dass alle vom Programm bereitgestellten Features oder Möglichkeiten mittels
Schaltflächen oder sonstigen komfortablen Möglichkeiten der Eingabe
intuitiv erreichbar und interaktiv sind. Dazu zählen z.B. die Nachfrage
der Applikation nach Parametern bzw. Argumenten für einen Funktionsaufruf,
die Bereitstellung wahrscheinlicher Standardeingabewerte, oder auch die
Überprüfung von Eingaben auf Zulässigkeit. So ist zum Beispiel die
Eingabe einer gewünschten Menge häufig nur für Werte größer als Null
und nur bis zu einer gewissen Höhe sinnvoll, aber ob ein Programmierer
auch entsprechende Tests bezüglich der Eingabe vorgesehen hat, ist
eine andere Frage.

Weiterhin sollten alle Schaltflächen, die eine Oberfläche bereitstellt,
bei Betätigung eine Rückmeldung geben -- selbst dann und insbesonders, wenn die verlangte
Funktion im Moment aus irgendeinem Grund nicht ausführbar ist.
Wenn ein Knopf momentan nicht funktioniert, muss dies entweder sichtbar gemacht werden,
oder eine Nachricht muss dem Nutzer erläutern, wieso der Knopfdruck nichts bewirkt.
\glqq{}Blinde\grqq{} Schaltflächen ohne jegliche Reaktion sind ein Funktionsdefizit.
Ein Nutzer kann schließlich üblicherweise nicht erkennen, ob und wieso eine Funktion
zu einem bestimmten Zeitpunkt bzw. unter gewissen Umständen nicht durchführbar ist
und wieso. Die Nutzeroberfläche muss diese Umstände zumindest wiedergeben
und idealerweise auch gleich erläutern, um den Nutzer zur korrekteren oder gewünschten
Bedienungsweise zu leiten.

Die Korrektheit einer Oberfläche bezeichnet z.B. das Öffnen eines erwarteten
Unterfensters oder die Ausführung der gewählten Funktion als Reaktion auf eine
Nutzereingabe. Es wird nicht einfach nur verlangt, dass etwas passiert, es soll
auch etwas sein, das vom jeweiligen Schalter und dem Nutzungskontext abhängig ist.
Wenn dieser Effekt außerhalb der graphischen Oberfläche zu finden ist, z.B.
als Änderung einer Datei im Betriebssystem, muss der Test ebenfalls die Grenzen
der getesteten Oberfläche verlassen und die gewünschte Änderung nachprüfen.

Da für solche Überprüfung die Mitwirkung des jeweiligen Betriebssystems notwendig
ist, welches je nach lokaler Ausführung unterschiedliche Eigenschaften für neue
Anwendungsfenster oder Dateisysteme aufweisen kann, zählt dies zu den am schwersten
zu validierenden Eigenschaften. Eine auf mehreren Betriebssystemen oder gar je
nach Version desselben Betriebssystems unterschiedlich agierende Applikation muss auch
in jeder denkbaren Konfiguration getestet werden. Hier gilt allerdings der
Grundsatz des Testens: \glqq{}Die Abwesenheit von Fehlern kann nicht bewiesen werden.\grqq{}
\footnote{E. Dijkstra, The Humble Programmer, ACM Turing Lecture 1972}

Der naive Ansatz eines Testverfahrens ist simpel. Ein menschlicher Tester startet
das Programm und arbeitet eine Liste von Funktionen bzw. Tätigkeiten ab, die das
Programm unterstützen muss. Er nutzt die graphische Oberfläche, um Eingaben zu tätigen,
und stellt sicher, dass Schaltflächen die vom Nutzer erwarteten oder zumindest
dokumentierte Verhaltensmuster aufweisen. Selbst wenn eine Oberfläche Probleme aufweist,
werden diese häufig als akzeptabel eingestuft, wenn eine gewünschte Funktion
bei korrekter Bedienung ausführbar ist. Ein Beispiel für ein solches häufig nicht als kritisch 
eingestuftes Problem ist das temporäre Nichtantworten einer Programmoberfläche
aufgrund einer rechenintensiven internen Tätigkeit. Idealerweise ist eine
graphische Oberfläche tatsächlich nur eine Oberfläche und gibt Instruktionen als
verkapselte Signale an das unterliegende Programm weiter. Wenn also eine
in irgendeiner Form rechenintensive Anweisung erfolgt, sollte die Oberfläche
antwortfähig bleiben und lediglich vermerken, dass das unterliegende Programm
momentan beschäftigt ist und weitere Eingaben warten müssen.

Eine Erweiterung bzw. ein Komplement zu Funktionstests sind destruktive Ansätze.
Hierbei versucht ein Tester nicht, irgendwelche Funktionen korrekt durchzuführen,
sondern hat als einziges Ziel, die graphische Oberfläche in einen Fehlerzustand
zu überführen. Hierfür können z.B. schnelle, ziellose Eingaben dienen -- wildes Herumgeklicke
auf dem Bildschirm -- oder auch die gezielte Auswahl von falschen und ungültigen
Parametern und Eingaben, wo auch immer es geht. Die korrekt agierende
graphische Oberfläche muss dies alles verarbeiten können, ohne in einen nicht
mehr verlassbaren Fehlerzustand zu geraten. Dazu zählen z.B. Deadlocks,
Abstürze oder das Nichtakzeptieren folgender Eingaben.

\subsection{Automatisierbare Ansätze}

Menschliche Arbeitszeit ist allerdings teuer, diese Prüfverfahren sind üblicherweise
mit starker Wiederholung verbunden, und die Überprüfung selbst häufig trivial einfach.
Es bietet sich also an, dies zu automatisieren. Hierfür gibt es viele verfügbare Ansätze
und entsprechende Anwendungen. Es gibt sowohl quelloffene und kostenlose als auch proprietäre
und kommerzielle Lösungen. Grundsätzlich sind automatisierte Tests in zwei Kategorien einzuordnen:
Offline- oder Online-Tests. Hierzu mehr in Kapitel \ref{section:offoronlinetesting} auf Seite
\pageref{section:offoronlinetesting}. Menschliches Offline-Testen existiert natürlich auch,
es nennt sich beispielsweise \glqq{}Pair Programming\grqq{} oder \glqq{}Commit Review\grqq{}
und bedeutet im Grunde lediglich, dass ein oder mehrere andere Entwickler den geschriebenen Quellcode
lesen und nach Fehlern suchen.

Weitere Unterteilung automatisierter Tests erfolgt hauptsächlich durch Unterschiede
in der Bedienung und des jeweiligen Nutzers. Sogenannte \glqq{}Unit Tests\grqq{} werden
von den Entwicklern selbst verfasst und bezeichnen zusätzlichen Programmcode parallel
zum zu testenden Programm. Dieser zusätzliche Code soll lediglich die Funktion und Korrektheit
eines kleinen Bestandteils des Programms sicherstellen, insbesondere, wenn die Implementation
des Bestandteils geändert wird. Hierfür wird üblicherweise eine kleine Menge an Testeingabewerten
vorgesehen, für die erwartete bzw. korrekte Antworten definiert sind. Diese Testeingaben
werden dann beim Unit Test durchgeführt und die Antworten mit dem Soll verglichen.

Als konkretes Beispiel diene hier ein Programmiererwitz: Ein Tester betritt eine Kneipe.
Er bestellt ein Bier. Er bestellt -1 Biere. Er bestellt zehn Trilliarden Biere. Er bestellt n Biere.
Er verlässt die Kneipe. Er bestellt 2 Biere. Er betritt die Kneipe. Er bestellt ein Pferd. Er bestellt \glqq{}ioubiuzb\grqq{}.
Dieser Witz soll verdeutlichen, dass für gute Tests häufig unsinnige Eingaben notwendig sind,
die vom Programm dennoch auf korrekte Art und Weise behandelt werden müssen. So wäre es in diesem
Fall fehlerhaft, zehn Trilliarden Biere vorzubereiten, obwohl die Eingabe technisch gesehen zulässig erscheint.
Ebenso erfolgen Bestellungen, obwohl die Kneipe verlassen wurde. Ein korrekt agierendes
Programm sollte auf solche Widersprüche mit einer Fehlermeldung reagieren, aber nicht mit
fehlerhaftem Verhalten.

Entsprechende Tests gibt es auch für graphische Oberflächen, bei denen Code nicht zwangsläufig
einen Antwortwert zurückliefert. So kann z.B. ein Test darin bestehen, eine Schaltfläche zu betätigen,
und anschließend zu prüfen, ob ein erwartetes Fenster geöffnet wurde. Hier wird die Problematik
komplexer, da bei einem korrekt verkapselten Programm das Öffnen eines neuen Fensters asynchron
und damit technisch gesehen zu einem nicht vorhersehbaren Zeitpunkt erfolgt. Für praktische Zwecke
kann allerdings vorgegeben werden, dass für eine korrekte Eingabeverarbeitung die Öffnung dieses 
Fensters innerhalb eines gewissen Zeitraums erfolgen muss.

Solche Tests müssen ebenfalls von Entwicklern implementiert werden, was einen erheblichen
Zeitaufwand zusätzlich zur Entwicklung selbst bedeutet. Um dies zu optimieren, wurde ein neuer Ansatz erdacht: Anstatt
in Codeform vorgeben zu müssen, was das erwartete Verhalten einer graphischen Oberfläche ist,
wird ein naiver Testablauf durch einen Menschen bzw. Tester vom Computer aufgenommen. Diese Aufnahme der
vorgenommenen Eingaben kann dann beliebig oft wieder abgespielt werden. Die Ausgaben bzw.
Antworten des Programms wurden ebenso aufgenommen und können mit den Ausgaben eines späteren
Durchlaufs mit identischen Eingaben verglichen werden. Dies sind sogenannte 
\glqq{}Regressionstests\grqq{} \cite{regression}. Damit lässt sich vor allem sicherstellen,
dass bestehende Funktionalität einer Oberfläche durch spätere Änderungen nicht zerstört wird.

Die Vorteile sind offensichtlich: Es ist kein Entwickler beteiligt, sondern ein
darauf spezialisierter Tester, und ein einmal aufgenommener Testablauf kann ohne weitere
menschliche Beteiligung beliebig oft vorgenommen werden. Ein Nachteil ist allerdings, dass
nur die vom Tester aufgenommenen Aktionen getestet werden können, insbesondere können
meist die Parameter oder die Testreihenfolge nachträglich nicht verändert werden. Auch
muss bei einer Erweiterung oder Veränderung der bestehenden Oberfläche ein komplett neuer Test
erstellt werden, da der alte nicht mehr zutrifft bzw. Funktionen auslässt. Je nach genutztem
Test erhält ein Nutzer lediglich ein lapidares \glqq{}Test nicht bestanden\grqq{} oder
eine genaue Aufschlüsselung, welche Eingabe zu welchem Zeitpunkt zu einer fehlenden 
oder unerwarteten Reaktion geführt hat.

\subsection{Vollautomatische Ansätze}

Alle bisherigen Verfahren bezogen sich in irgendeiner Form auf menschliche Nutzung der
Oberflächen. Es gibt auch vollautomatische Ansätze, die nicht erfordern, dass ein Mensch
die korrekte Nutzung vorgibt. Diese nutzen meist Programmierschnittstellen,
auf denen graphische Oberflächen aufbauen. Diese Schnittstellen bieten Gemeinsamkeiten
über alle Programme hinweg, die zum vollautomatischen Testen verwendet werden können.
Dazu zählen insbesondere normierte Schaltflächen und sonstige Eingabemöglichkeiten
sowie die Erkennung sich öffnender Fenster. Ein vollautomatischer Test ist also zumindest
in der Lage, Schalter zu betätigen und neue Fenster zu beobachten.

Das Problem vollautomatischer Tests ist, dass Maschinen keine Intelligenz aufweisen,
um irgendeinen Kontext zu erkennen oder herstellen zu können. Ein Programm hat
keine Möglichkeit, festzustellen, ob ein sich öffnendes Fenster in einer Oberfläche
auch das gewünschte Fenster ist, ob es sich zum richtigen Zeitpunkt öffnet, oder
ob es überhaupt die korrekten Inhalte umfasst. Ebenso kann ohne Kontext nicht festgestellt
werden, dass ein bestimmter Schalter nur dann funktioniert, wenn vorher Parameter
in andere Eingabefelder eingegeben wurden, oder dass diese Parameter innerhalb bestimmter
Grenzen liegen müssen.

Für vollautomatische Tests bietet sich daher der bereits erwähnte destruktive Ansatz an.
Es gibt keine sinnvolle Möglichkeit, Funktion oder Korrektheit ohne Kontext zu testen,
aber um mögliche Fehler und Ausnahmen herbeizuführen, ist das auch unnötig.
Der Test versucht nun also, mit vielen, falschen, ungültigen oder auch unsinnigen
Eingaben das zu testende Programm zu überwältigen. Das Ziel ist es, nicht korrekt
behandelte Situationen zu finden, Kontrollflüsse also, in denen von falschen
Annahmen bezüglich des Programmzustands und der Eingaben ausgegangen wird.

Und obwohl solch ein vollautomatischer Test idealerweise keinerlei Angaben
über das zu testende Programm benötigt, sind in der Praxis doch Anpassungen
naheliegend. So ist es wenig zweckdienlich, wieder und wieder ein- und 
denselben Knopf zu testen,
der das Programm beendet und damit auch den Test. Eine völlig zufällige
Testanordnung würde zwar dennoch zwangsläufig irgendwann alle Möglichkeiten
ausgeschöpft haben, aber dieses simple Eingeständnis verringert die
Anzahl nötiger Testläufe erheblich.

Weiterhin kann jede Anwendung Bedienelemente enthalten, deren Betätigung
den weiteren Test sinnlos macht -- als Beispiel sei eine Login-Maske genannt,
in der falsche Eingaben schlicht zur Beendigung des Programms führen.
Ein anderes Beispiel wären z.B. Schaltflächen wie
\glqq{}Projekt schließen\grqq{}, \glqq{}Alles löschen\grqq{} oder auch 
\glqq{}globaler thermonuklearer Krieg\grqq{}.


\section{Automatisierte GUI-Tests}\label{section:automatedguitesting}


\textbf{uispec4j} \footnote{\url{ https://github.com/UISpec4J/UISpec4J }} ist ein Java-Programm 
zum Test von graphischen Nutzeroberflächen
in Java. Es wird ein Programm implementiert, welches parallel zur zu testenden GUI läuft,
vorgegebene Eingaben vornimmt und im Anschluss einen gewünschten Zustand in den Elementen der GUI
überprüfen kann. Ein Problem ist dabei, dass nur oberflächliche Dinge geprüft werden können,
wie z.B. die Existenz oder das Fehlen eines fest definierten Fensters oder Elements. Weiterhin muss für jede
Prüfanfrage ein Entwickler ein entsprechendes Testsegment schreiben. Die Applikation muss also
zumindest doppelt implementiert werden, was in einem hohen Aufwand resultiert. Auch muss bei jeder
Refaktorisierung oder Änderung des Programms auch jeder anhängige Test angepasst werden.
Dies kann bei modernen Entwicklungsverfahren mit kurzen Iterations-Intervallen zu Problemen führen,
oder zumindest den Prozess verlangsamen.


\vspace{0.5cm}

\textbf{fest} \footnote{\url{ https://code.google.com/p/fest/ }} oder 
\glqq{}Fixtures for Easy Software Testing\grqq{} verfolgt denselben Ansatz.
Es agiert als Kontrollprogramm auf einer zu testenden Applikation und kann gewisse Prüfungen
auf dieser ausführen. Tatsächlich sind die beiden praktisch identisch, bauen sie doch beide
auf \textbf{JUnit} \footnote{\url{ http://junit.org/ }} auf und erweitern dieses.
uispec4j wird allerdings nicht mehr aktiv weiterentwickelt und bietet
beispielsweise keine Unterstützung für das Java Runtime Environment Version \glqq{}1.7\grqq{}
oder höher. Beide greifen auf die Java-Bibliothek \glqq{}Robot\grqq{} zurück,
die zur Durchführung von Tests einen menschlichen Nutzer simuliert. Dieser 
bewegt die Maus, klickt an vorprogrammierten Koordinaten mit dem Mauszeiger
(dies wird teilweise durch automatische Positionserkennung von definierten Elementen
der getesteten Oberflächen verbessert) und führt vordefinierte Tastatureingaben durch.


\vspace{0.5cm}

\textbf{QF-Test} \footnote{\url{ http://www.qfs.de/en/qftest/index.html }} 
bietet darüber hinaus eine eigene graphische Nutzeroberfläche
und ein sogenanntes \glqq{}Capture / Replay\grqq{}-System. Hierbei soll es Entwicklern erspart bleiben,
Tests als Programme zu implementieren, stattdessen bedient ein Tester durch die QF-Test-Maske
hindurch ein zu prüfendes Programm, während die Eingaben und Ausgaben mitgeschnitten werden.
Diese Aufnahme des Programmverhaltens gilt dann als \glqq{}Sollverhalten\grqq{} und wird in Folge
durch automatisierte Wiederholung verglichen und geprüft. Dieselbe Serie von Eingaben muss
bei Testdurchläufen eine identische Ausgabe zu diesem Soll hervorrufen, ansonsten
wird ein Fehler im Programm vermutet. Dieser Fehler kann auch eine gewünschte Programmveränderung sein.

Da hier allerdings ein Tester und nicht ein Entwickler die Tests erstellt und pflegt, ist dieses
Verfahren arbeitszeittechnisch wesentlich effizienter als vorherige Methoden. Ein Tester muss
nicht die internen Abläufe bzw. Zustände des Programms bzw. des zu testenden Programmsegments 
verstehen oder verstehen können. Er muss das Programm lediglich
bedienen können. Ebenso sind keine Programmierkenntnisse notwendig. Der Tester folgt sogenannten
\glqq{}On-Screen\grqq{}-Anweisungen, die Schritt für Schritt durch die korrekte Aufnahme einer
Nutzungsabfolge für spätere automatische Wiedergabe leiten. Das Resultat ist ein typischer
Regressionstest. Spätere Iterationen desselben Programms werden mit denselben Eingaben
bedient, und es wird unterstellt, dass auch dieselben (korrekten) Ausgaben erfolgen.
Stimmen gefundene Ausgaben in der Zukunft nicht mit der ursprünglichen Aufnahme überein, resultiert
dies in einer Fehlermeldung seitens des Testers. Dies tritt z.B. ein, wenn eine Refaktorisierung,
also eine Änderung bestehender Quellcode-Strukturen, in mehr als den erwarteten Änderungen
resultiert und bestehende Funktionalität zerstört oder beeinträchtigt. Dies tritt bei
gemeinsam genutzten Funktionen im Code schnell auf, insbesonders dann, wenn eine Implementation 
auf nicht dokumentierten Eigenarten bestimmter Anweisungen setzt, um ihr Ziel zu erreichen.

Undokumentierte Nebeneffekte überleben selten eine Refaktorisierung oder Reimplementierung
in derselben Form. Eine Lösung ist, das gewünschte Verhalten möglichst vollständig zu dokumentieren
und darüber hinaus mit Regressionstests zu überprüfen.


\vspace{0.5cm}

\textbf{Maveryx} \footnote{\url{ http://www.maveryx.com/en/products/what-is-maveryx.html }} 
ist ein weiteres Programm für die automatische Durchführung von Tests auf Nutzeroberflächen.
Eine Besonderheit, mit der hier geworben wird, ist die Unnötigkeit der Aufnahme
eines Testdurchlaufs bzw. einer Struktur der GUI für die Durchführung anschließender
Tests. Prinzipiell funktioniert Maveryx in drei Segmenten: Es gibt den sogenannten
\glqq{}Finder\grqq{}, welcher eine GUI während der Laufzeit mithilfe der
\glqq{}Java Accessibility Layer\grqq{} (hiermit sind vermutlich AWT, Swing, SWT etc. gemeint)
in seine Elemente aufschlüsselt und diese in einem XML-Baumformat abspeichert.
Das nächste Segment, der \glqq{}Viewer\grqq{} sowie das letzte, der \glqq{}Robot\grqq{}
arbeiten zusammen, um zunächst mithilfe von Textsuche ein zu testendes Element in
dem vom Finder erstellten XML-Baum zu finden, im Viewer darzustellen und mithilfe
des Robots pseudo-menschliche Nutzereingaben zu tätigen.

Anstatt die zu prüfenden Elemente also durch Nutzung und Aufnahme der Eingaben
oder auch direkt programmatisch und hochspezifisch vorzugeben, würde ein Tester bei
Maveryx den Finder über das zu testende Programm laufen lassen. Die resultierende
XML-Datei würde dann begutachtet, um sich Testfälle anhand der Elementnamen bzw. sonstiger
textueller Informationen, die über ein Element gewonnen werden können, zu überlegen.
Etwas umgangssprachlicher: Anstelle der Vorgabe \glqq{}das Passwortfeld P, welches
durch Betätigung der Knöpfe A und B im Fenster X sichtbar wird\grqq{} als Ziel
eines Tests würde man das Passwortfeld P in der vom Maveryx-Finder erstellten
XML-Datei suchen, es hätte vielleicht eine Bezeichnung \glqq{}PPasswort\grqq{},
und lediglich diese für zukünftige Tests in einem Maveryx-Testskript speichern.
Das Testprogramm wäre dann selbsttätig in der Lage, ein Passwortfeld P,
welches dieselbe Bezeichnung aufweist, in zukünftigen veränderten Versionen
der zu testenden Applikation zu finden, ungeachtet beliebiger Änderungen bzw.
Weiterentwicklungen der graphischen Nutzeroberfläche.

Es sei allerdings zu bedenken, dass dieses Konzept nur dann aufgeht, wenn
die Elemente der zu testenden Nutzeroberfläche vom verantwortlichen Programmierer
für den Tester sinnvoll erscheinend benannt wurden. Wenn z.B. die Elemente
einfach nummeriert wurden, oder von chinesischen oder indischen Subkontraktoren
in ihren jeweiligen Sprachen sinnvoll benannt, könnte es schwierig werden,
in der erstellten XML-Datei die zu testenden Elemente zu finden. Maveryx scheint
eine Vorgabe der zu prüfenden Objekte zur Laufzeit durch Anklicken oder
Auswahl nicht anzubieten. Auch stellt sich die Frage, ob ein Tester ohne weiteres
in der Lage ist, die von Maveryx verlangte Skriptsprache zur Testdefinition
zu erlernen -- es handelt sich schließlich quasi um eine Programmiersprache.
Wenn man einen Entwickler zur Bedienung benötigt, ist einer der Hauptvorteile
automatisierter Tests fraglich.

Es wird weiterhin damit geworben, dass kein Bildschirm und keine Bildverarbeitung
notwendig ist, um die Tests durchzuführen. Maveryx setze auf ein 
Konzept der \glqq{}Computer Vision\grqq{} zur robusten Umsetzung der
Tests. Dies erscheint angesichts der doch händisch anmutenden Methodik
der Testvorgabe per XML-Datei als eine hoch gegriffene Behauptung. Auch stellt
sich die Frage, ob ohne einen zumindest simulierten Bildschirm ein vollständiger
Test einer graphischen Oberfläche machbar oder sinnvoll ist. Zur Computer
Vision gibt es aber keine weiteren Informationen, und die Demonstrationsvideos
zeigen auch nur manuell angestoßene Testreihen auf einem nutzergesteuerten Rechner.
Auch scheint das Konzept einzelne Fenster spezifisch für einen Test zu instanziieren,
was dem Test einer vollständigen laufenden Applikation widersprechen würde. Dieses
Verfahren ähnelt eher einem Unit-Test, wo eine einzelne Komponente abgeschnitten
vom Rest des Programms gestartet und getestet wird.


\vspace{0.5cm}

\subsection{Kontinuierliche Qualitätskontrolle von Webanwendungen auf Basis 
maschinengelernter Modelle}\label{ssection:windmueller}


Die \glqq{}Dissertation Kontinuierliche Qualitätskontrolle von Webanwendungen auf 
Basis maschinengelernter Modelle\grqq{} \cite{diss:windmueller} stellt ein neuartiges Konzept
zur Überprüfung des Verhaltens einer Applikation vor. Die Idee ist, mithilfe eines
lernenden Algorithmus nach Angluin \cite{angluin} einen Automaten bzw. ein komplettes Modell
der dem Algorithmus unbekannten, quasi \glqq{}Black Box\grqq{}, Anwendung zu erstellen und zu vergleichen.

Hierzu muss aber erwähnt werden, dass lediglich der das Modell erstellende Algorithmus nicht
weiss, was das Programm macht - seitens des Testers ist erheblicher Aufwand für die Implementation
von sogenannten Treibern notwendig, die jede mögliche Eingabe an die Anwendung bzw. jedes
mögliche Wort des Eingabealphabets abdeckt. Ebenso müssen die bei Webanwendungen normierten
Antworten auf die Eingaben gelesen und für Zustandsabbildung genutzt werden. Diese Treiber
sind hochspezifisch für eine Anwendung und müssen sogar für verschiedene Versionen derselben
Anwendung neu angepasst werden.

Zurück zum Prinzip: Als Resultat existieren nun die erlernten Modelle der Anwendung bzw. spezifischer
Versionen der Anwendung. Diese kann man nun mit einem Algorithmus vergleichen und Unterschiede
herausstellen. Solche entstehen letztendlich nur aus zwei Gründen: Die Anwendung wurde verändert
und es tritt eine gewünschte Funktionsänderung oder -erweiterung auf, oder ein neuartiger Fehler
wurde (im Vergleich zur letzten Version) erschaffen. Diese Differenz ist offensichtlich nur
aus dem Kontext heraus und nur durch einen Menschen zu verstehen.

Insofern handelt es sich um eine Form des Regressionstests \cite{regression} - bestehende bzw.
vorherige Funktion wird mit einem Momentanzustand verglichen und Diskrepanzen aufgedeckt. Der
Ansatz hat dementsprechend dieselben Nachteile - jede gewünschte Änderung des Systems muss eine
Neueinstellung oder Neuerstellung der Testkonfiguration nach sich ziehen. Bei häufigen
Änderungen kann der Aufwand der Testerstellung schnell dem Entwicklungsaufwand gleichkommen bzw.
diesen sogar überschreiten.

Der Vorteil ist natürlich genauso, dass selbst geringfügige Abweichungen im Verhalten sofort
auffallen und zur Analyse markiert werden. So wurden beim Praxiseinsatz auf verschiedenen
Webanwendungen und ihren Versionskontrollsystemen bis dato unerkannte Programmfehler entdeckt.
Diese wurden durch Eigenarten und Konzepte der bestehenden Tests maskiert und unsichtbar.
Eine offensichtliche Schlussfolgerung ist, dass mehr und verschiedene Tests zu besseren
Ergebnissen führen - das Problem liegt, wie immer, im Aufwand.

Verglichen mit einem gewöhnlichen Regressionstest erspart dieser Ansatz dem Nutzer
bzw. Entwickler die Definition der Zustandsfolgen. Zu beachten ist allerdings wieder:
Es gibt für den Algorithmus kein richtig oder falsch, nur Unterschiede zwischen Versionen.
Ein normaler GUI-Test definiert üblicherweise eine erwartete Antwort oder Reaktion auf einen
bestimmten Knopfdruck. Hier würde so lange kein \glqq{}Problem\grqq{} auftreten, wie das Verhalten eines
Programms konsistent ist, selbst wenn dieses Verhalten an sich eigentlich fehlerhaft ist.

Es gibt noch ein weiteres für das hier vorgestellte Konzept interessantes Feature,
den sogenannten reuse-Filter. Da ein Automat mit einer endlichen Zustandsmenge unterstellt
wird und gelernt werden soll (der Ansatz würde versagen, wenn das Programm nicht in dieses
Schema fällt), muss eine bestimmte Eingabe von einem bestimmten Zustand aus erfolgen.
Diesen Zustand allein durch Eingaben wieder zu erreichen - unter Umständen nach einem Programmneustart -
kann erhebliche Verzögerungen im Testablauf hervorrufen.

Die Idee ist folglich, Zustände des
zu testenden Programms zu sichern und dann aus dem Speicher wiederherzustellen. Dies setzt
natürlich voraus, dass dies möglich ist und man es für das Programm implementiert.
Für den hier vorgestellten Ansatz und allgemeiner eine Java Virtual Machine ist dies nicht
ohne weiteres möglich. Java-Programme sind zustandsmäßig unüberschaubar und können auch nicht
grundsätzlich in einem bestimmten Zustand abgespeichert werden. Die Alternative
eines kompletten virtuellen Rechners mit einer eingefrorenen JVM darin wäre
zumindest sehr aufwändig und vermutlich nicht sehr performant.

Weitere mögliche Nebeneffekte, die bei den getesteten Programmen nicht vorkamen, sind
Zustandsveränderungen auf einer persistenten Datenbank oder einem Speichermedium, die sogar
Beendigung des zu testenden Programms selbst überdauern. Hierfür müsste eine externe
Lösung gefunden werden, zum Beispiel das Sichern eines gesamten Systemzustands in einer
virtuellen Maschine und das jeweilige Neustarten eines Testablaufs von diesem gesicherten
Zustand aus.


\vspace{0.5cm}

Im Vergleich mit Dr. Windmüllers Ansatz wird das hier vorgestellte Konzept erheblich weniger
Kontextinformationen benötigen. So lange keine programmspezifischen, mit der Java-Swing-API
inkompatiblen Implementationen vorliegen, sollte ein Testdurchlauf ohne jede Information abseits
des Startpunktes möglich sein. Es gibt kein Eingabealphabet, welches erst für ein zu testendes
Programm definiert werden müsste.

Des Weiteren haben Java-GUI-Anwendungen den Nachteil, nicht auf jede Eingabe antworten oder gar
reagieren zu müssen. Eine Reaktion tritt nicht einmal im selben Kontrollfluss auf, obwohl Swing
konzeptuell monolithisch abläuft. Ebenso ist die Definition eines Zustandes selbst nicht trivial,
der interne Programmzustand ist unüberschaubar. Lediglich das Auftreten und die Erscheinung neuer
Fenster und graphischer Programmkomponenten kann beobachtet und verzeichnet werden.



\subsection{An Empirical Study of the Robustness of Windows NT Applications Using Random Testing}\label{ssection:windmueller}


Diese etwas ältere Arbeit von Forrester und Miller \cite{winNTforrester} befasst sich 
mit dem Verhalten einer Auswahl von Anwendungen im Betriebssystem Windows, wenn zufällige 
Daten als Eingaben verwendet wurden.
Es wurden sowohl Anwendungen mit und ohne grafischer Nutzeroberfläche getestet. Der Ansatz hierbei
ist Black-Box; es gibt keinerlei Wissen über Inhalt, Zweck oder Verhalten der zu testenden Software.
Als zufällige Eingaben dienen sowohl gültige Signale von Tastatur und Maus, wie ein Nutzer sie erzeugen
könnte, sowie Windows-spezifische, sogenannte \glqq{}Win32\grqq{}-Signale. Diese sind auch im aktuellen Windows 10
nach wie vor im Einsatz, aufgrund der Abwärtskompatibilität vermutlich sogar in nahezu identischer Form.

Forrester und Miller nennen ihr Vorgehen \glqq{}Fuzz Testing\grqq{}. Vor Windows wandten sie dasselbe Verfahren
in zwei Vorgänger-Arbeiten auf Linux-Anwendungen an. Bei gängigen Applikationen zeigte sich, dass zwischen
ein Viertel und ein Drittel der geprüften Anwendungen nicht mit den zufälligen Eingabedaten zurecht kam.
Das einzige Kriterium für den \glqq{}Erfolg\grqq{} ist die Abnahme der Eingabe sowie ein ordnungsgemäßes Beenden
des Programms -- selbst wenn dies lediglich eine sofortige Ausgabe einer Fehlermeldung ist.

Die Eingabe von zufälligen Maus- und Tastatursignalen ist definitiv einem realen Anwendungsfall zuzuordnen,
dieser Fall könnte schliesslich genau so auch auftreten. Die zufälligen Win32-Signale testen eher die
allgemeine Stabilität bzw. Sicherheit, die Fehlererkennung, eines Programms.

Eine Eingabe mittels Maus oder Tastatur löst zunächst eine Prozessor-Unterbrechung aus. Die Unterbrechung
leitet die Eingabedaten an den jeweiligen Gerätetreiber weiter, welcher den Inhalt der Nachricht ausliest
(welche Taste wurde gedrückt, wo befindet sich der Mauszeiger etc.). Dies löst ein Win32-Ereignis
aus. Das Betriebssystem stellt dann fest, welche Applikation das Ziel der Eingabe war, und kopiert
das Ereignis in den Ereignis-Eingang dieser Applikation. Anwendungen haben üblicherweise eine interne
Endlossschleife, welche diesen Eingang regelmäßig auf neue Nachrichten überprüft, diese ausliest und das
Programm entsprechend reagieren lässt. Obwohl im Normalfall davon ausgegangen werden kann, dass ein System
nur gültige Win32-Nachrichten verschickt, sollte ein Programmierer nicht davon absehen, dies auch
zu kontrollieren. Ein Angreifer könnte ansonsten ein Fehlverhalten des Programms bei ungültigen Eingaben
ausnutzen, es könnten Sicherheitslücken auftreten ö.Ä..

Zu beachten ist allerdings, dass \glqq{}Fuzz\grqq{} eine gewaltige Anzahl von Eingaben praktisch gleichzeitig tätigt
(zehntausende). Man könnte zu Recht argumentieren, dass ein normaler Anwendungsfall eine solche Menge von
Eingaben in einer kurzen Zeit nicht vorsieht, und damit vorsätzlich interne Puffer zur 
Verarbeitung von Eingaben überlastet werden
könnten. Modernere Versionen derselben Applikationen bzw. desselben Betriebssystems könnten diesen Fall 
unter Umständen besser abfangen.

\vspace{0.5cm}


Zu den Ergebnissen: Getestet wurden verschiedene bekannte Applikationen der Firmen Adobe und Microsoft sowie Mozilla -- es
finden sich der Acrobat Reader, Office, Internet Explorer sowie der Netscape Navigator 
(Vorläufer von Firefox) in der Liste. Getestet wurde unter Windows NT und Windows 2000.

21\% der getesteten Applikationen stürzten ab, wenn zufällige, gültige Maus- und Tastatureingaben 
getätigt wurden. Weitere 24\% versagten in Form von Endlosschleifen. Dies ist eine Fehlerrate von 45\%
allein für völlig legale und im Zweifelsfall möglicherweise auftretende Eingaben.

Im Fall der zufälligen (auch ungültigen) Win32-Signale lag die Fehlerrate bei nahezu 100\%. Dies ist
laut den Autoren, die den Quellcode einiger Applikationen einsehen konnten, damit zu erklären,
dass Programmierer von der Verlässlichkeit der empfangenen Signale ausgehen (diese Garantie ist offensichtlich
nicht gegeben). Die Autoren nennen dies eine grobe Schwachstelle der Win32-API, insbesondere bezüglich
Sicherheit, da schließlich jedes Programm der Systemebene diese Win32-Signale beliebiger Zusammensetzung
an andere Programme versenden kann. Dies könnte in neueren Windows-Versionen anders sein, das Betriebssystem
könnte eine automatische Fehlerkorrektur der Win32-Nachrichten vornehmen.

Für das hier vorgestellte Konzept stellen sich im Bezug auf diese Arbeit einige Fragen:
Wie schnell sollten Eingaben erfolgen? Müssen Eingaben zwangsläufig funktional korrekt erfolgen?
Auf welcher Abstraktionsebene erfolgen die Eingaben und welche Möglichkeiten zur Datenmanipulation
gibt es überhaupt? Wird die Nachstellung eines bösartigen Angriffs beabsichtigt?


%%% TeX-master: "../main.tex"
% kapitel3.tex
\section{\glqq{}Vollautomatische\grqq{} GUI-Tests}\label{chapter:introfullautoguitesting}


In diesem Kapitel werden existierende GUI-Testverfahren betrachtet, welche das Prädikat
\glqq{}vollautomatisch\grqq{} zumindest teilweise für sich beanspruchen. Diese Behauptung wird
auf Wahrheitsgehalt überprüft, eventuelle Fallstricke, Nachteile und Einschränkungen werden aufgezeigt
und Schlussfolgerungen für das hier vorgestellte Konzept werden gezogen, soweit möglich.



\subsection{GUITest: a Java library for fully automated GUI robustness testing}

Existierende Arbeiten analogen Inhalts sind z.B. \textbf{\glqq{}GUITest: a Java library for fully automated
GUI robustness testing\grqq{}} \cite{GUITestBauersfeld}: Eine Java-Bibliothek 
(allerdings speziell für das Betriebssystem Mac OSX entwickelt), die
vollautomatisch mögliche Eingaben in einer beliebigen (allerdings hier nur an einer getesteten)
Applikation der \glqq{}Mac OSX Accessibility API\grqq{} aufspürt und durchführt.

Ähnlich wie bei der Java Swing API existiert hierbei eine objektorientierte Baumstruktur,
der sogenannte \glqq{}Widget Tree\grqq{} bzw. Widget-Baum, welcher alle Elemente der graphischen Oberfläche enthält
und insbesondere genormte Klassen und Schnittstellen zum Beispiel für Schalter
und Knöpfe enthält. Dies erlaubt eine vollautomatische Verarbeitung:
Jede tatsächlich implementierte Funktion lässt sich durch ein vorher definiertes
\glqq{}Eingabealphabet\grqq{} erreichen und auslösen.

Ein Unterschied zum
hier vorgestellten Konzept ist, dass das Tool nicht auf die Applikation beschränkt sein müsste. Es wird
explizit erwähnt, dass die Shutdown- und Reset-Tasten des Host-Betriebssystems ausgenommen werden
mussten. Dieser Umstand ist damit zu erklären, dass das Betriebssystem Mac OSX eine allgemeine
Schnittstelle für graphische Anwendungsoberflächen bereitstellt und vermutlich auch voraussetzt,
dass für OSX zertifizierte Anwendungen diese implementieren. Folglich handelt es sich um eine
Testapplikation allgemein für OSX-Programme, welche allerdings hier nur mit einem Programm
getestet wurde. Das in dieser Arbeit implementierte Konzept ist für das im direkten Vergleich 
wesentlich allgemeingültigere Java Swing ausgelegt und kann auf jeder Plattform ausgeführt werden, 
die Unterstützung für eine JVM bietet. Mac OSX zählt auch dazu.

Eine Gemeinsamkeit bzw. logische Konsequenz des vollautomatischen Testvorgangs ist, dass nur
die Robustheit getestet werden kann. Ohne Kontext ist es nicht möglich, dem Verhalten eines
Programms Begriffe wie \glqq{}korrekt\grqq{} oder \glqq{}falsch\grqq{} zuzuordnen. Lediglich auftretende
Fehler und der Eingabeweg bis zu Ihrem Auftreten sind von Interesse.
Die möglichen Eingaben durch die \glqq{}Mac OSX Accessibility API\grqq{} sind stärker normiert als bei
Java Swing, wodurch weniger Variation möglich ist. Auch prüft dieses Programm nicht mögliche
Textfeldeingaben wie das hier vorgestellte Konzept, wobei dies seitens der OSX-API möglich gewesen wäre.

Anderere Unterschiede finden sich im Vorgehen: Die Autoren wünschen einen vollautomatischen
Testvorgang, scheuen sich aber nicht, programmspezifische Implementationen vorzunehmen, um
bestimmte kontextuelle Features zu testen. Als Beispiel dient hier das Ziehen, sogenanntes
\glqq{}Click-And-Drag\grqq{}, von Clip-Art aus der Word-Kommandoleiste in das Word-Dokument.
Sie räumen ein, dass ihre Applikation lediglich ein Standard-Set von Aktionen für
normierte Eingabeverarbeiter ableitet und für ein besseres Testergebnis gewisse
kontextuelle Aktionen notwendig werden. So wird der Tester in jedem Fall angewiesen,
ein Word-Dokument zu öffnen (ein vollautomatisches Werkzeug würde dies nur durch Zufall erreichen),
darüber hinaus arbeitet der Tester in einem vom Betriebssystem beschränkten Useraccount,
um Änderungen bzw. Schäden am Betriebssystem durch zufällige Eingaben zu verhindern.

Von den neun zum Absturz führenden Eingabesequenzen, die GUITest schließlich fand,
konnten die Autoren drei nicht selbst bzw. manuell reproduzieren und zitieren zeitabhängigkeit
der Eingaben, die den Absturz hervorrufen sollen. Dies impliziert, dass die gewählten Warte-Intervalle
zu kurz gewählt waren, um wirklich menschliche Eingaben nachstellen zu können. Wenn es
menschlich machbare Eingaben gewesen wären, 
könnten Menschen die resultierenden Abstürze auch reproduzierbar.
Hieraus ist die Lehre zu ziehen, bei durch Menschen zeitlich nicht machbaren Fehlern die Toleranzen
des Testprogramms so lange zu lockern, bis der Fehler nicht mehr auftritt oder ein Mensch
denselben Fehler überhaupt erzeugen kann.


\subsection{A GUI Crawling-Based Technique for Android Mobile Application Testing}

Ein anderes Beispiel wäre \textbf{\glqq{}A GUI Crawling-Based Technique for Android Mobile Application
Testing\grqq{}}\cite{AGCBTFAMAT}. Hierbei wird in zwei separaten Schritten zunächst eine graphische Applikation
vollautomatisch auf mögliche Eingaben und anzeigbare Bildschirme durchsucht, also ein
\glqq{}Wegnetz\grqq{} ähnlich dem Vorschlagskonzept durch das Programm gelegt, bzw.
ein Modell des Programms inferiert oder generiert, um dann in einem zweiten
Schritt mit üblichen (regressiven) Test-Applikationen und Vorgehensweisen Abläufe zu
planen und tatsächlich zu prüfen. Der Unterschied und die Einschränkung hier ist
hauptsächlich die Android-GUI-API sowie die Testumgebung eines Android-Emulators am
PC-System. Auch ist der zweite Vorgehensschritt nicht vollautomatisch und erfordert in jedem Fall
erheblichen Kontext bzw. Nutzerangaben.

Zu beachten ist die Besonderheit, dass der Crawler-Bestandteil der Applikation nicht
einfach nur nach möglichen Eingabekomponente sucht, sondern tatsächlich die
eingabeverarbeitenden Komponenten, die \glqq{}Event Handler\grqq{}, identifiziert
und Rückschlüsse daraus zieht. So werden die möglichen Eingabetypen ausgelesen --
dies ist für ein Android-Gerät wichtig, da mehr als nur einfache \glqq{}Klicks\grqq{}
möglich sein könnten, wie Gestik oder gar sensorische Eingaben wie das Schütteln
des Geräts. Auf einem PC sind diese Eingaben zwar seit der Einführung von
Desktop-Touchscreens theoretisch möglich, aber es existiert keine normierte
Definition abseits des Android-Umfelds und folglich ignoriert diese Arbeit
eventuell vorhandene spezifische Implementationen.

Aus den so gefundenen möglichen Eingabewerten wird zufällig gewählt,
die Applikation ist also nichtdeterministisch. Eine Besonderheit von Android
kommt den Autoren entgegen: Popups oder seperate Fenster existieren nicht.
Wenn eine Applikation die graphische Darstellung verändert, enthält entweder
der aktuelle Komponentenbaum die veränderten Komponenten, oder der
Komponentenbaum selbst wurde geändert -- die Applikation ist in einem neuen Fenster,
welches den gesamten Bildschirm des Geräts einnimmt.
Der von Android erzwungenermaßen implementierte \glqq{}Zurück\grqq{}-Knopf ist
eine weitere Annehmlichkeit. Es besteht keine Notwendigkeit, mühsam einen Zustand wieder
von vorne herzustellen, wenn auch einfach von einem Folgezustand rückwärts
gegangen werden kann.

Ein Einsatz des vorgestellten Konzepts auf Android ist zwar theoretisch möglich, jedoch setzt
man dort eine spezielle Variante von Swing für die graphischen Oberflächen ein, 
wodurch eine Anpassung bzw. Erweiterung des Programms sehr wahrscheinlich notwendig wäre. 
Auch stellt das Android-Betriebssystem seine eigenen Herausforderungen
bezüglich Nutzbarkeit und insbesondere auch Programminteraktion -- das Konzept
einer Android-Applikation als virtueller alleiniger Nutzer seines eigenen Systems
erschwert offensichtlich jegliche programmübergreifende Tätigkeiten.

Der Testansatz der Autoren vergleicht insbesondere auch die Instruktionen in den gefundenen
eingabeverarbeitenden Komponenten -- dies ist etwas, was das hier vorgestellte Konzept nicht
vorsieht. Hier wird nur mit den Daten gearbeitet, nicht mit Instruktionen. Abweichende
Instruktionen (verglichen mit vorherigen Durchläufen) werden als mögliche Fehler markiert.
Die Autoren weisen darauf hin, dass ihr Test so auch in der graphischen Oberfläche
unsichtbare Fehler in der Implementation aufdecken kann. Dagegen gehalten werden kann,
dass viele beliebige und insbesondere gewollte Änderungen von Instruktionen in
der Applikation so als Regression markiert werden. Dies führt dazu, dass eher ein
Differenz-Analysetool vorliegt als ein tatsächlicher Fehlertester.

Die Autoren weisen auch darauf hin, dass der Ansatz komplett zufälliger Eingaben
suboptimal ist -- eine gewisse Auswahl von häufig Fehlern erzeugenden Eingaben
bzw. klassenartige Prüfung wäre vermutlich eine effektivere Strategie, um Fehler
herbeizuführen.


\subsection{Monkey und monkeyrunner}

Ebenfalls speziell für Android konzipiert sind die Werkzeuge
\textbf{Monkey}\footnote{ \url{ https://developer.android.com/tools/help/monkey.html }}
und \textbf{monkeyrunner}
\footnote{ \url{ https://developer.android.com/tools/help/monkeyrunner_concepts.html }}.
Hierbei handelt es sich von Google selbst für Entwickler bereitgestellte
Schnittstellen und Applikationen, die z.B. für automatische Tests 
der von ihnen entwickelten Anwendungen für Android dienen können oder sollen.
Ersteres ist hierbei als \glqq{}stress-test\grqq{} bezeichnet und letzteres
als \glqq{}Fernsteuerung durch eine API für Geräte und Emulatoren von 
einem Arbeitsplatz aus\grqq{}. Monkey wird mittels der Android Debug Bridge
\footnote{ \url{ https://developer.android.com/tools/help/adb.html }}
bzw. \textbf{adb} direkt auf dem zu testenden Gerät ausgeführt. Es erzeugt
einen pseudozufälligen Strom von Ereignissen wie beispielsweise
Berührungen und Gestiken seitens eines Nutzers oder auch Systemeregnisse,
welche vom Android-Gerät selbst ausgehen würden.

Im Betrieb beobachtet Monkey das System
des Gerätes und hält Ausschau nach drei Konditionen: Erstens, der
Kontrollfluss wird auf die von Monkey zu prüfenden Klassenpfade bzw.
Pakete beschränkt. Jeder Versuch, den aktuellen Kontrollfluss in ein
anderes Paket bzw. ein anderes Programm zu leiten wird unterbunden.
Zweitens, sollte die getestete Applikation eine unbehandelte Ausnahme
erzeugen, stoppt Monkey den Testvorgang und gibt die Fehlermeldung der
Ausnahme aus. Drittens wird die Reaktion der getesteten Anwendung
kontinuerlich validiert -- sollte ein Fehler des Typs \glqq{}Anwendung
reagiert nicht mehr\grqq{} auftreten, bleibt Monkey ebenfalls stehen
und erstattet Bericht. Zu beachten ist, dass Monkey nicht selbsttätig
oder gezielt Bildschirmwechsel vornimmt oder auch nur irgendwie
auf die Inhalte der getesteten Applikation eingeht -- einfach gesagt ist es egal,
ob Knöpfe vorhanden sind oder nicht, es werden einfach 
mittels zufälligen Koordinaten auf dem gesamten Bildschirm Eingaben 
vorgenommen. Dies ist im Rahmen von Android-Anwendungen
eine zulässige Teststrategie, da typischerweise die gesamte Oberfläche
eines beliebigen Bildschirms der Bedienung einem einzigen Programm dient.
Auch ist die Reaktion der Applikation abgesehen von den erwähnten
Fehlern und Ausnahmezuständen uninteressant, also muss auch nicht auf
diese gewartet werden. Wenn der Entwickler bestimmte Anwendungsfälle
oder -bildschirme testen will, muss Monkey programmatisch dorthin
geleitet werden. Abgesehen von den zufälligen Eingaben verhält sich
Monkey also doch wie klassischer automatischer Unit-Test.

Monkeyrunner ist nicht eine Erweiterung oder Weiterentwicklung von Monkey,
sondern komplett unabhängig. Im Vergleich zu Monkey hat es viel eher die
Features üblicher Testsuites: Es kann mehrere Android-Geräte
gleichzeitig kontrollieren, diese können physikalisch angeschlossene oder
auf in Emulatoren virtualisierte Instanzen sein. Auf diesen kann parallel
oder auch nacheinander ein Testprogramm durchgeführt werden. Monkeyrunner
erlaubt, einen programmatisch definierten Eingabeablauf vollautomatisch
durchzuführen und nimmt währenddessen Screenshots des Geräts auf.
Als besondere Variante des Regressionstests können die Aufnahmen eines
als \glqq{}korrekt\grqq{} bekannten Durchlaufs mit folgenden Bildserien
verglichen werden, um so visuelle Differenzen herauszustellen. Dies sagt
natürlich wenig über die internen Zustände eines Programms aus, aber
man kann argumentieren, dass nur für den Nutzer sichtbare Reaktionen
für den Test einer graphischen Oberfläche von Interesse sind. Ein durch
Regression abweichendes Verhalten der Applikation bzw. Ergebnis einer
Bedienungssequenz wird so sehr schnell gefunden. Die für Regressionstests
üblichen Schwierigkeiten bestehen -- bei einer gewünschten Programmänderung
muss ein vollständiger neuer Sollzustand definiert werden, um zukünftige
Durchläufe damit vergleichen zu können.

Es bietet weiterhin modular erweiterbare Automation mittels Python-basierter
Module und Anwendungen an. Python\footnote{ \url{ https://www.python.org/ }} 
ist eine dynamisch kompilierte Skriptsprache, die schnelle Entwicklungsprozesse
und Iteration erleichtern soll. In dieser Hinsicht ist monkeyrunner zwar
als automatisches Testwerkzeug nutzbar, dies ist aber nicht sein einziger
Zweck bzw. seine Spezialität. Die Bedienung erfordert aber in jedem Fall
einen Entwickler, oder wäre einem nicht mit Python-Programmierung vertrauten 
Tester nur schwer zuzumuten.

