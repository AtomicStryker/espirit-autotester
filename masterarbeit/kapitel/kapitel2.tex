%%% TeX-master: "../main.tex"
% kapitel2.tex
\chapter{Funktionstests graphischer Oberflächen}\label{chapter:introguitesting}


Dieses Kapitel befasst sich mit einigen in der Industrie verbreiteten bzw. \glqq{}State of the Art\grqq{}-Lösungen
für das Problem automatischer GUI-Tests, dazu mit in einem Artikel sowie einer Doktorarbeit
vorgestellten Methoden. 
Es werden verschiedene Lösungen (für Java-Swing und andere) vorgestellt,
ihr jeweiliger Implementationsaufwand begutachtet sowie eventuelle Vor- und Nachteile aufgezeigt.
Auch wenn bereits im Ansatz ein Unterschied zum hier vorgestellten Konzept besteht, kann man dennoch
Aufwand und Nutzen sowie Vor- und Nachteile vergleichen.


\section{Grundlagen: Unterschiede Testverfahren}\label{section:testingapproaches}


Grundsätzlich haben Tests von graphischen Oberflächen das Ziel, sowohl die
Funktion als auch die Korrektheit zu überprüfen. Zum Bereich Funktion zählt,
dass alle vom Programm bereitgestellten Features oder Möglichkeiten mittels
Schaltflächen oder sonstigen komfortablen Möglichkeiten der Eingabe
intuitiv erreichbar und interaktiv sind. Dazu zählen z.B. die Nachfrage
der Applikation nach Parametern bzw. Argumenten für einen Funktionsaufruf,
die Bereitstellung wahrscheinlicher Standardeingabewerte, oder auch die
Überprüfung von Eingaben auf Zulässigkeit.

Weiterhin sollten alle Schaltflächen, die eine Oberfläche bereitstellt,
bei Betätigung eine Rückmeldung geben -- selbst dann und insbesonders, wenn die verlangte
Funktion im Moment aus irgendeinem Grund nicht ausführbar ist.
Wenn ein Knopf momentan nicht funktioniert, muss dies entweder sichtbar gemacht werden,
oder eine Nachricht muss dem Nutzer erläutern, wieso der Knopfdruck nichts bewirkt.
\glqq{}Blinde\grqq{} Schaltflächen ohne jegliche Reaktion sind ein Funktionsdefizit.

Die Korrektheit einer Oberfläche bezeichnet z.B. das Öffnen eines erwarteten
Unterfensters oder die Ausführung der gewählten Funktion als Reaktion auf eine
Nutzereingabe. Es wird nicht einfach nur verlangt, dass etwas passiert, es soll
auch etwas sein, dass vom jeweiligen Schalter und dem Nutzungskontext abhängig ist.
Wenn dieser Effekt ausserhalb der graphischen Oberfläche zu finden ist, z.B.
als Änderung einer Datei im Betriebssystem, muss der Test ebenfalls die Grenzen
der getesteten Oberfläche verlassen und die gewünschte Änderung validieren.

Da für solche Überprüfung die Mitwirkung des jeweiligen Betriebssystems notwendig
ist, welches je nach lokaler Ausführung unterschiedliche Eigenschaften für neue
Anwendungsfenster oder Dateisysteme aufweisen kann, zählt dies zu den am schwersten
zu validierenden Eigenschaften. Eine auf mehreren Betriebssystemen oder gar je
nach Version desselben Betriebssystems unterschiedlich agierende Applikation muss auch
in jeder denkbaren Konfiguration getestet werden. Hier gilt allerdings der
Grundsatz des Testens: Die Abwesenheit von Fehlern kann nicht bewiesen werden.

Der naive Ansatz eines Testverfahrens ist simpel. Ein menschlicher Tester startet
das Programm und arbeitet eine Liste von Funktionen bzw. Tätigkeiten ab, die das
Programm unterstützen muss. Er nutzt die graphische Oberfläche, um Eingaben zu tätigen,
und stellt sicher, dass Schaltflächen die vom Nutzer erwarteten oder zumindest
dokumentierte Verhaltensmuster aufweisen. Selbst wenn eine Oberfläche Probleme aufweist,
werden diese häufig als akzeptabel eingestuft, wenn eine gewünschte Funktion
bei Korrekter Bedienung ausführbar ist. Ein Beispiel für ein solches häufig nicht als kritisch 
eingestuftes Problem ist das temporäre Nichtantworten einer Programmoberfläche
aufgrund einer rechenintensiven internen Tätigkeit. Idealerweise ist eine
graphische Oberfläche tatsächlich nur eine Oberfläche und gibt Instruktionen als
verkapselte Signale an das unterliegende Programm weiter. Wenn also eine
in irgendeiner Form rechenintensive Anweisung erfolgt, sollte die Oberfläche
antwortfähig bleiben und lediglich vermerken, dass das unterliegende Programm
momentan beschäftigt ist und weitere Eingaben warten müssen.

Eine Erweiterung bzw. ein Komplement zu Funktionstests sind destruktive Ansätze.
Hierbei versucht ein Tester nicht, irgendwelche Funktionen korrekt durchzuführen,
sondern hat als einziges Ziel, die graphische Oberfläche in einen Fehlerzustand
zu überführen. Hierfür können z.B. schnelle, ziellose Eingaben dienen -- wildes Herumgeklicke
auf dem Bildschirm -- oder auch die gezielte Auswahl von falschen und ungültigen
Parametern und Eingaben, wo auch immer es geht. Die korrekt agierende
graphische Oberfläche muss dies alles verarbeiten können, ohne in einen nicht
mehr verlassbaren Fehlerzustand zu geraten. Dazu zählen z.B. Deadlocks,
Abstürze oder das Nichtakzeptieren folgender Eingaben.

\subsection{Automatisierbare Ansätze}

Menschliche Arbeitszeit ist allerdings teuer, diese Prüfverfahren sind üblicherweise
mit starker Wiederholung verbunden, und die Überprüfung selbst häufig trivial einfach.
Es bietet sich also an, dies zu automatisieren. Hierfür gibt es viele verfügbare Ansätze
und entsprechende Anwendungen. Es gibt sowohl quelloffene und kostenlose als auch proprietäre
und kommerzielle Lösungen. Grundsätzlich sind automatisierte Tests in zwei Kategorien einzuordnen:
Offline- oder Online-Tests. Hierzu mehr in Kapitel \ref{section:offoronlinetesting} auf Seite
\pageref{section:offoronlinetesting}. Menschliches Offline-Testen existiert natürlich auch,
es nennt sich beispielsweise \glqq{}Pair Programming\grqq{} oder \glqq{}Commit Review\grqq{}
und bedeutet im Grunde lediglich, dass ein oder mehrere andere Entwickler den geschriebenen Quellcode
lesen und nach Fehlern suchen.

Weitere Unterteilung automatisierter Tests erfolgt hauptsächlich durch Unterschiede
in der Bedienung und des jeweiligen Nutzers. Sogenannte \glqq{}Unit Tests\grqq{} werden
von den Entwicklern selbst verfasst und bezeichnen zusätzlichen Programmcode parallel
zum zu testenden Programm. Dieser zusätzliche Code soll lediglich die Funktion und Korrektheit
eines kleinen Bestandteils des Programms sicherstellen, insbesondere, wenn die Implementation
des Bestandteils geändert wird. Hierfür wird üblicherweise eine kleine Menge an Testeingabewerten
vorgesehen, für die erwartete bzw. korrekte Antworten definiert sind. Diese Testeingaben
werden dann beim Unit Test durchgeführt und die Antworten mit dem Soll verglichen.

Als konkretes Beispiel diene hier ein Programmiererwitz: Ein Tester betritt eine Kneipe.
Er bestellt ein Bier. Er bestellt -1 Biere. Er bestellt zehn Trilliarden Biere. Er bestellt n Biere.
Er verlässt die Kneipe. Er bestellt 2 Biere. Er betritt die Kneipe. Er bestellt ein Pferd. Er bestellt \glqq{}ioubiuzb\grqq{}.
Dieser Witz soll verdeutlichen, dass für gute Tests häufig unsinnige Eingaben notwendig sind,
die vom Programm dennoch auf korrekte Art und Weise behandelt werden müssen. So wäre es in diesem
Fall fehlerhaft, zehn Trilliarden Biere vorzubereiten, obwohl die Eingabe technisch gesehen zulässig erscheint.
Ebenso erfolgen Bestellungen, obwohl die Kneipe verlassen wurde. Ein korrekt agierendes
Programm sollte auf solche Widersprüche mit einer Fehlermeldung reagieren, aber nicht mit
fehlerhaftem Verhalten.

Entsprechende Tests gibt es auch für graphische Oberflächen, bei denen Code nicht zwangsläufig
einen Antwortwert zurückliefert. So kann z.B. ein Test darin bestehen, eine Schaltfläche zu betätigen,
und anschliessend zu prüfen, ob ein erwartetes Fenster geöffnet wurde. Hier wird die Problematik
komplexer, da bei einem korrekt verkapselten Programm das Öffnen eines neuen Fensters asynchron
und damit technisch gesehen zu einem nicht vorhersehbaren Zeitpunkt erfolgt. Für praktische Zwecke
kann allerdings vorgegeben werden, dass für eine korrekte Eingabeverarbeitung die Öffnung dieses 
Fensters innerhalb eines gewissen Zeitraums erfolgen muss.

Solche Tests müssen ebenfalls von Entwicklern implementiert werden, was einen erheblichen und
teuren Zeitaufwand bedeutet. Um dies zu optimieren, wurde ein neuer Ansatz erdacht: Anstatt
in Codeform vorgeben zu müssen, was das erwartete Verhalten einer graphischen Oberfläche ist,
wird ein naiver Testablauf durch einen Menschen bzw. Tester vom Computer aufgenommen. Diese Aufnahme der
vorgenommenen Eingaben kann dann beliebig oft wieder abgespielt werden. Die Ausgaben bzw.
Antworten des Programms wurden ebenso aufgenommen und können mit den Ausgaben eines späteren
Durchlaufs mit identischen Eingaben verglichen werden. Dies sind sogenannte 
\glqq{}Regressionstests\grqq{} \cite{regression}. Damit lässt sich vor allem sicherstellen,
dass bestehende Funktionalität einer Oberfläche durch spätere Änderungen nicht zerstört wird.

Die Vorteile sind offensichtlich: Es ist kein teurer Entwickler beteiligt, lediglich ein
erheblich günstigerer Tester, und ein einmal aufgenommener Testablauf kann ohne weitere
menschliche Beteiligung beliebig oft vorgenommen werden. Ein Nachteil ist allerdings, dass
nur die vom Tester aufgenommenen Aktionen getestet werden können, insbesondere können
meist die Parameter oder die Testreihenfolge nachträglich nicht verändert werden. Auch
muss bei einer Erweiterung oder Veränderung der bestehenden Oberfläche ein komplett neuer Test
erstellt werden, da der alte nicht mehr zutrifft bzw. Funktionen auslässt. Je nach genutztem
Test erhält ein Nutzer lediglich ein lapidares \glqq{}Test nicht bestanden\grqq{} oder
eine genaue Aufschlüsselung, welche Eingabe zu welchem Zeitpunkt zu einer fehlenden 
oder unerwarteten Reaktion geführt hat.

\subsection{Vollautomatische Ansätze}

Alle bisherigen Verfahren bezogen sich in irgendeiner Form auf menschliche Nutzung der
Oberflächen. Es gibt auch vollautomatische Ansätze, die nicht erfordern, dass ein Mensch
die korrekte Nutzung vorgibt. Diese nutzen meist Programmierschnittstellen aus,
auf denen graphische Oberflächen aufbauen. Diese Schnittstellen bieten Gemeinsamkeiten
über alle Programme hinweg, die zum vollautomatischen Testen verwendet werden können.
Dazu zählen insbesondere normierte Schaltflächen und sonstige Eingabemöglichkeiten
sowie die Erkennung sich öffnender Fenster. Ein vollautomatischer Test ist also zumindest
in der Lage, Schalter zu betätigen und neue Fenster zu beobachten.

Das Problem vollautomatischer Tests ist, dass Maschinen keine Intelligenz aufweisen,
um irgendeinen Kontext zu erkennen oder herstellen zu können. Ein Programm hat
keine Möglichkeit, festzustellen, ob ein sich öffnendes Fenster in einer Oberfläche
auch das gewünschte Fenster ist, ob es sich zum richtigen Zeitpunkt öffnet, oder
ob es überhaupt die korrekten Inhalte umfasst. Ebenso kann ohne Kontext nicht festgestellt
werden, dass ein bestimmter Schalter nur dann funktioniert, wenn vorher Parameter
in andere Eingabefelder eingegeben wurden, oder dass diese Parameter innerhalb bestimmter
Grenzen liegen müssen.

Für vollautomatische Tests bietet sich daher der bereits erwähnte destruktive Ansatz an.
Es gibt keine sinnvolle Möglichkeit, Funktion oder Korrektheit ohne Kontext zu testen,
aber um mögliche Fehler und Ausnahmen herbeizuführen, ist das auch unnötig.
Der Test versucht nun also, mit vielen, falschen, ungültigen oder auch unsinnigen
Eingaben das zu testende Programm zu überwältigen. Das Ziel ist es, nicht korrekt
behandelte Situationen zu finden, Kontrollflüsse also, in denen von falschen
Annahmen bezüglich des Programmzustands und der Eingaben ausgegangen wird.

Und obwohl solch ein vollautomatischer Test idealerweise keinerlei Angaben
über das zu testende Programm benötigt, sind in der Praxis doch Anpassungen
naheliegend. So macht es wenig Sinn, wieder und wieder einen Knopf zu testen,
der das Programm beendet und damit auch den Test. Eine völlig zufällige
Testanordnung würde zwar dennoch zwangsläufig irgendwann alle Möglichkeiten
ausgeschöpft haben, aber dieses simple Eingeständnis verringert die
Anzahl nötiger Testläufe erheblich.

Weiterhin kann jede Anwendung Bedienelemente enthalten, dessen Betätigung
den weiteren Test sinnlos macht -- als ein Beispiel sei eine Login-Maske genannt,
in der falsche Eingaben schlicht zur Beendigung des Programms führen.
Ein anderes Beispiel wären z.B. Schaltflächen wie
\glqq{}Projekt schließen\grqq{}, \glqq{}Alles löschen\grqq{} oder auch 
\glqq{}globaler thermonuklearer Krieg\grqq{}.




\section{Automatisierte GUI-Tests}\label{section:automatedguitesting}


\textbf{uispec4j} \footnote{\url{ https://github.com/UISpec4J/UISpec4J }} ist ein Java-Programm 
zum Test von graphischen Nutzeroberflächen
in Java. Es wird ein Programm implementiert, welches parallel zur zu testenden GUI läuft,
vorgegebene Eingaben vornimmt und im Anschluss einen gewünschten Zustand in den Elementen der GUI
überprüfen kann. Ein Problem ist dabei, dass nur oberflächliche Dinge geprüft werden können,
wie z.B. die Existenz oder das Fehlen eines fest definierten Fensters oder Elements. Andererseits muss für jede
Prüfanfrage ein Entwickler ein entsprechendes Testsegment schreiben. Die Applikation muss also
zumindest doppelt implementiert werden, was in einem hohen Aufwand resultiert.

\vspace{0.5cm}

\textbf{fest} \footnote{\url{ https://code.google.com/p/fest/ }} oder 
\glqq{}Fixtures for Easy Software Testing\grqq{} verfolgt denselben Ansatz.
Es agiert als Kontrollprogramm auf einer zu testenden Applikation und kann gewisse Prüfungen
auf dieser ausführen. Tatsächlich sind die beiden praktisch identisch, bauen sie doch beide
auf \textbf{JUnit} \footnote{\url{ http://junit.org/ }} auf und erweitern dieses.

\vspace{0.5cm}

\textbf{QF-Test} \footnote{\url{ http://www.qfs.de/en/qftest/index.html }} 
bietet darüber hinaus eine eigene graphische Nutzeroberfläche
und ein sogenanntes \glqq{}Capture / Replay\grqq{}-System. Hierbei soll es Entwicklern erspart bleiben,
Tests als Programme zu implementieren, stattdessen bedient ein Tester durch die QF-Test-Maske
hindurch ein zu prüfendes Programm, während die Eingaben und Ausgaben mitgeschnitten werden.
Diese Aufnahme des Programmverhaltens gilt dann als \glqq{}Sollverhalten\grqq{} und wird in Folge
durch automatisierte Wiederholung verglichen und geprüft. Dieselbe Serie von Eingaben muss
bei Testdurchläufen eine identische Ausgabe zu diesem Soll hervorrufen, ansonsten
wird ein Fehler im Programm vermutet. Dieser Fehler kann auch eine gewünschte Programmveränderung sein.

Da allerdings hier ein Tester und nicht ein Entwickler die Tests erstellt und pflegt, ist dieses
Verfahren arbeitszeittechnisch wesentlich effizienter als vorherige Methoden. Ein Tester muss
nicht die Innereien des Programms verstehen oder verstehen können. Er muss das Programm lediglich
bedienen können. Ebenso sind keine Programmierkenntnisse notwendig.


\subsection{Kontinuierliche Qualitätskontrolle von Webanwendungen auf Basis 
maschinengelernter Modelle}\label{ssection:windmueller}


Die \glqq{}Dissertation Kontinuierliche Qualitätskontrolle von Webanwendungen auf 
Basis maschinengelernter Modelle\grqq{} \cite{diss:windmueller} stellt ein neuartiges Konzept
zur Überprüfung des Verhaltens einer Applikation vor. Die Idee ist, mithilfe eines
lernenden Algorithmus nach Angluin \cite{angluin} einen Automaten bzw. ein komplettes Modell
der dem Algorithmus unbekannten, quasi \glqq{}Black Box\grqq{}, Anwendung zu erstellen und zu vergleichen.

Hierzu muss aber erwähnt werden, dass lediglich der das Modell erstellende Algorithmus nicht
weiss, was das Programm macht - seitens des Testers ist erheblicher Aufwand für die Implementation
von sogenannten Treibern notwendig, die jede mögliche Eingabe an die Anwendung bzw. jedes
mögliche Wort des Eingabealphabets abdeckt. Ebenso müssen die bei Webanwendungen normierten
Antworten auf die Eingaben gelesen und für Zustandsabbildung genutzt werden. Diese Treiber
sind hochspezifisch für eine Anwendung und müssen sogar für verschiedene Versionen derselben
Anwendung neu angepasst werden.

Zurück zum Prinzip: Als Resultat existieren nun die erlernten Modelle der Anwendung bzw. spezifischer
Versionen der Anwendung. Diese kann man nun mit einem Algorithmus vergleichen und Unterschiede
herausstellen. Solche entstehen letztendlich nur aus zwei Gründen: Die Anwendung wurde verändert
und es tritt eine gewünschte Funktionsänderung oder -erweiterung auf, oder ein neuartiger Fehler
wurde (im Vergleich zur letzten Version) erschaffen. Diese Differenz ist offensichtlich nur
aus dem Kontext heraus und nur durch einen Menschen zu verstehen.

Insofern handelt es sich um eine Form des Regressionstests \cite{regression} - bestehende bzw.
vorherige Funktion wird mit einem Momentanzustand verglichen und Diskrepanzen aufgedeckt. Der
Ansatz hat dementsprechend dieselben Nachteile - jede gewünschte Änderung des Systems muss eine
Neueinstellung oder Neuerstellung der Testkonfiguration nach sich ziehen. Bei häufigen
Änderungen kann der Aufwand der Testerstellung schnell dem Entwicklungsaufwand gleichkommen bzw.
diesen sogar überschreiten.

Der Vorteil ist natürlich genauso, dass selbst geringfügige Abweichungen im Verhalten sofort
auffallen und zur Analyse markiert werden. So wurden beim Praxiseinsatz auf verschiedenen
Webanwendungen und ihren Versionskontrollsystemen bis dato unerkannte Programmfehler entdeckt.
Diese wurden durch Eigenarten und Konzepte der bestehenden Tests maskiert und unsichtbar.
Eine offensichtliche Schlussfolgerung ist, dass mehr und verschiedene Tests zu besseren
Ergebnissen führen - das Problem liegt, wie immer, im Aufwand.

Verglichen mit einem gewöhnlichen Regressionstest erspart dieser Ansatz dem Nutzer
bzw. Entwickler die Definition der Zustandsfolgen. Zu beachten ist allerdings wieder:
Es gibt für den Algorithmus kein richtig oder falsch, nur Unterschiede zwischen Versionen.
Ein normaler GUI-Test definiert üblicherweise eine erwartete Antwort oder Reaktion auf einen
bestimmten Knopfdruck. Hier würde so lange kein \glqq{}Problem\grqq{} auftreten, wie das Verhalten eines
Programms konsistent ist, selbst wenn dieses Verhalten an sich eigentlich fehlerhaft ist.

Es gibt noch ein weiteres für das hier vorgestellte Konzept interessantes Feature,
den sogenannten reuse-Filter. Da ein Automat mit einer endlichen Zustandsmenge unterstellt
wird und gelernt werden soll (der Ansatz würde versagen, wenn das Programm nicht in dieses
Schema fällt), muss eine bestimmte Eingabe von einem bestimmten Zustand aus erfolgen.
Diesen Zustand allein durch Eingaben wieder zu erreichen - unter Umständen nach einem Programmneustart -
kann erhebliche Verzögerungen im Testablauf hervorrufen.

Die Idee ist folglich, Zustände des
zu testenden Programms zu sichern und dann aus dem Speicher wiederherzustellen. Dies setzt
natürlich voraus, dass dies möglich ist und man es für das Programm implementiert.
Für den hier vorgestellten Ansatz und allgemeiner eine Java Virtual Machine ist dies nicht
ohne weiteres möglich. Java-Programme sind zustandsmäßig unüberschaubar und können auch nicht
grundsätzlich in einem bestimmten Zustand abgespeichert werden. Die Alternative
eines kompletten virtuellen Rechners mit einer eingefrorenen JVM darin wäre
zumindest sehr aufwändig und vermutlich nicht sehr performant.

Weitere mögliche Nebeneffekte, die bei den getesteten Programmen nicht vorkamen, sind
Zustandsveränderungen auf einer persistenten Datenbank oder einem Speichermedium, die sogar
Beendigung des zu testenden Programms selbst überdauern. Hierfür müsste eine externe
Lösung gefunden werden, zum Beispiel das Sichern eines gesamten Systemzustands in einer
virtuellen Maschine und das jeweilige Neustarten eines Testablaufs von diesem gesicherten
Zustand aus.

\vspace{0.5cm}

Im Vergleich mit Dr. Windmüllers Ansatz wird das hier vorgestellte Konzept erheblich weniger
Kontextinformationen benötigen. So lange keine programmspezifischen, mit der Java-Swing-API
inkompatiblen Implementationen vorliegen, sollte ein Testdurchlauf ohne jede Information abseits
des Startpunktes möglich sein. Es gibt kein Eingabealphabet, welches erst für ein zu testendes
Programm definiert werden müsste.

Des Weiteren haben Java-GUI-Anwendungen den Nachteil, nicht auf jede Eingabe antworten oder gar
reagieren zu müssen. Eine Reaktion tritt nicht einmal im selben Kontrollfluss auf, obwohl Swing
konzeptuell monolithisch abläuft. Ebenso ist die Definition eines Zustandes selbst nicht trivial,
der interne Programmzustand ist unüberschaubar. Lediglich das Auftreten und die Erscheinung neuer
Fenster und graphischer Programmkomponenten kann beobachtet und verzeichnet werden.



\subsection{An Empirical Study of the Robustness of Windows NT Applications Using Random Testing}\label{ssection:windmueller}


Diese etwas ältere Arbeit von Forrester und Miller \cite{winNTforrester} befasst sich 
mit dem Verhalten einer Auswahl von Anwendungen im Betriebssystem Windows, wenn zufällige 
Daten als Eingaben verwendet wurden.
Es wurden sowohl Anwendungen mit und ohne grafischer Nutzeroberfläche getestet. Der Ansatz hierbei
ist Black-Box; es gibt keinerlei Wissen über Inhalt, Zweck oder Verhalten der zu testenden Software.
Als zufällige Eingaben dienen sowohl gültige Signale von Tastatur und Maus, wie ein Nutzer sie erzeugen
könnte, sowie Windows-spezifische, sogenannte \glqq{}Win32\grqq{}-Signale. Diese sind auch im aktuellen Windows 10
nach wie vor im Einsatz, aufgrund der Abwärtskompatibilität vermutlich sogar in nahezu identischer Form.

Forrester und Miller nennen ihr Vorgehen \glqq{}Fuzz Testing\grqq{}. Vor Windows wandten sie dasselbe Verfahren
in zwei Vorgänger-Arbeiten auf Linux-Anwendungen an. Bei gängigen Applikationen zeigte sich, dass zwischen
ein Viertel und ein Drittel der geprüften Anwendungen nicht mit den zufälligen Eingabedaten zurecht kam.
Das einzige Kriterium für den \glqq{}Erfolg\grqq{} ist die Abnahme der Eingabe sowie ein ordnungsgemäßes Beenden
des Programms -- selbst wenn dies lediglich eine sofortige Ausgabe einer Fehlermeldung ist.

Die Eingabe von zufälligen Maus- und Tastatursignalen ist definitiv einem realen Anwendungsfall zuzuordnen,
dieser Fall könnte schliesslich genau so auch auftreten. Die zufälligen Win32-Signale testen eher die
allgemeine Stabilität bzw. Sicherheit, die Fehlererkennung, eines Programms.

Eine Eingabe mittels Maus oder Tastatur löst zunächst eine Prozessor-Unterbrechung aus. Die Unterbrechung
leitet die Eingabedaten an den jeweiligen Gerätetreiber weiter, welcher den Inhalt der Nachricht ausliest
(welche Taste wurde gedrückt, wo befindet sich der Mauszeiger etc.). Dies löst ein Win32-Ereignis
aus. Das Betriebssystem stellt dann fest, welche Applikation das Ziel der Eingabe war, und kopiert
das Ereignis in den Ereignis-Eingang dieser Applikation. Anwendungen haben üblicherweise eine interne
Endlossschleife, welche diesen Eingang regelmäßig auf neue Nachrichten überprüft, diese ausliest und das
Programm entsprechend reagieren lässt. Obwohl im Normalfall davon ausgegangen werden kann, dass ein System
nur gültige Win32-Nachrichten verschickt, sollte ein Programmierer nicht davon absehen, dies auch
zu kontrollieren. Ein Angreifer könnte ansonsten ein Fehlverhalten des Programms bei ungültigen Eingaben
ausnutzen, es könnten Sicherheitslücken auftreten ö.Ä..

Zu beachten ist allerdings, dass \glqq{}Fuzz\grqq{} eine gewaltige Anzahl von Eingaben praktisch gleichzeitig tätigt
(zehntausende). Man könnte zu Recht argumentieren, dass ein normaler Anwendungsfall eine solche Menge von
Eingaben in einer kurzen Zeit nicht vorsieht, und damit vorsätzlich interne Puffer zur 
Verarbeitung von Eingaben überlastet werden
könnten. Modernere Versionen derselben Applikationen bzw. desselben Betriebssystems könnten diesen Fall 
unter Umständen besser abfangen.

\vspace{0.5cm}


Zu den Ergebnissen: Getestet wurden verschiedene bekannte Applikationen der Firmen Adobe und Microsoft sowie Mozilla -- es
finden sich der Acrobat Reader, Office, Internet Explorer sowie der Netscape Navigator 
(Vorläufer von Firefox) in der Liste. Getestet wurde unter Windows NT und Windows 2000.

21\% der getesteten Applikationen stürzten ab, wenn zufällige, gültige Maus- und Tastatureingaben 
getätigt wurden. Weitere 24\% versagten in Form von Endlosschleifen. Dies ist eine Fehlerrate von 45\%
allein für völlig legale und im Zweifelsfall möglicherweise auftretende Eingaben.

Im Fall der zufälligen (auch ungültigen) Win32-Signale lag die Fehlerrate bei nahezu 100\%. Dies ist
laut den Autoren, die den Quellcode einiger Applikationen einsehen konnten, damit zu erklären,
dass Programmierer von der Verlässlichkeit der empfangenen Signale ausgehen (diese Garantie ist offensichtlich
nicht gegeben). Die Autoren nennen dies eine grobe Schwachstelle der Win32-API, insbesondere bezüglich
Sicherheit, da schließlich jedes Programm der Systemebene diese Win32-Signale beliebiger Zusammensetzung
an andere Programme versenden kann. Dies könnte in neueren Windows-Versionen anders sein, das Betriebssystem
könnte eine automatische Fehlerkorrektur der Win32-Nachrichten vornehmen.

Für das hier vorgestellte Konzept stellen sich im Bezug auf diese Arbeit einige Fragen:
Wie schnell sollten Eingaben erfolgen? Müssen Eingaben zwangsläufig funktional korrekt erfolgen?
Auf welcher Abstraktionsebene erfolgen die Eingaben und welche Möglichkeiten zur Datenmanipulation
gibt es überhaupt? Wird die Nachstellung eines bösartigen Angriffs beabsichtigt?
